import{_ as e,o as t,c as o,Q as i}from"./chunks/framework.fd8801d6.js";const a="/assets/lr_1.d757091c.png",s="/assets/lr_1.d757091c.png",f=JSON.parse(`{"title":"Logistic Regression: From Odds to Evens in Data's Playground","description":"","frontmatter":{},"headers":[],"relativePath":"articles/education/machina_learning/logistic-regression.md","filePath":"articles/education/machina_learning/logistic-regression.md"}`),l={name:"articles/education/machina_learning/logistic-regression.md"},r=i('<h1 id="logistic-regression-from-odds-to-evens-in-data-s-playground" tabindex="-1">Logistic Regression: From Odds to Evens in Data&#39;s Playground <a class="header-anchor" href="#logistic-regression-from-odds-to-evens-in-data-s-playground" aria-label="Permalink to &quot;Logistic Regression: From Odds to Evens in Data&#39;s Playground&quot;">â€‹</a></h1><blockquote><h1 id="é€»è¾‘å›å½’-æ•°æ®æ¸¸ä¹åœºä¸­çš„ä»èµ”ç‡åˆ°å¶æ•°" tabindex="-1">é€»è¾‘å›å½’ï¼šæ•°æ®æ¸¸ä¹åœºä¸­çš„ä»èµ”ç‡åˆ°å¶æ•° <a class="header-anchor" href="#é€»è¾‘å›å½’-æ•°æ®æ¸¸ä¹åœºä¸­çš„ä»èµ”ç‡åˆ°å¶æ•°" aria-label="Permalink to &quot;é€»è¾‘å›å½’ï¼šæ•°æ®æ¸¸ä¹åœºä¸­çš„ä»èµ”ç‡åˆ°å¶æ•°&quot;">â€‹</a></h1></blockquote><p>Hello data adventurers! ğŸ’ Today, we&#39;re about to embark on a journey into the realm of logistic regression, a classic yet powerful tool in the data scientist&#39;s toolkit. Through the lens of logistic regression, we&#39;ll explore how we can make sense of the chaotic world of data, especially when we&#39;re dealing with binary outcomes - a yes or a no, a win or a loss, a 1 or a 0. ğŸ”„</p><blockquote><p>æ•°æ®å†’é™©å®¶ä»¬ï¼Œå¤§å®¶å¥½ï¼ğŸ’ ä»Šå¤©ï¼Œæˆ‘ä»¬å³å°†è¸ä¸Šé€»è¾‘å›å½’é¢†åŸŸçš„æ—…ç¨‹ï¼Œè¿™æ˜¯æ•°æ®ç§‘å­¦å®¶å·¥å…·åŒ…ä¸­ç»å…¸è€Œå¼ºå¤§çš„å·¥å…·ã€‚é€šè¿‡é€»è¾‘å›å½’çš„è§†è§’ï¼Œæˆ‘ä»¬å°†æ¢ç´¢å¦‚ä½•ç†è§£æ··ä¹±çš„æ•°æ®ä¸–ç•Œï¼Œå°¤å…¶æ˜¯å½“æˆ‘ä»¬å¤„ç†äºŒå…ƒç»“æœæ—¶â€”â€”æ˜¯æˆ–å¦ã€èµ¢æˆ–è¾“ã€1 æˆ– 0ã€‚ğŸ”„</p></blockquote><h2 id="history-å†å²" tabindex="-1">History å†å² <a class="header-anchor" href="#history-å†å²" aria-label="Permalink to &quot;History å†å²&quot;">â€‹</a></h2><p>Our tale begins in the early 19th century with a genius mathematician from Belgium named Pierre FranÃ§ois Verhulst. In 1838, Verhulst introduced the world to the logistic function through his publication, &quot;Correspondance mathÃ©matique et physique&quot;. The logistic function was like a key that could unlock the complexity of growth processes, especially populations. ğŸŒğŸ”</p><blockquote><p>æˆ‘ä»¬çš„æ•…äº‹å§‹äº 19 ä¸–çºªåˆï¼Œä¸€ä½åå«çš®åŸƒå°”Â·å¼—æœ—ç´¢ç“¦Â·éŸ¦å°”æ–¯ç‰¹ ï¼ˆPierre FranÃ§ois Verhulstï¼‰ çš„æ¯”åˆ©æ—¶å¤©æ‰æ•°å­¦å®¶ã€‚1838 å¹´ï¼ŒVerhulst é€šè¿‡ä»–çš„å‡ºç‰ˆç‰©â€œCorrespondance mathÃ©matique et physiqueâ€å‘ä¸–ç•Œä»‹ç»äº†ç‰©æµåŠŸèƒ½ã€‚ç‰©æµåŠŸèƒ½å°±åƒä¸€æŠŠé’¥åŒ™ï¼Œå¯ä»¥è§£å¼€å¢é•¿è¿‡ç¨‹çš„å¤æ‚æ€§ï¼Œå°¤å…¶æ˜¯äººå£ã€‚ğŸŒğŸ”</p></blockquote><p>Fast forward to the 20th century, the baton of logistic regression was picked up by Joseph Berkson. He modernized logistic regression, making it a staple in the statistical realm from 1944 onwards. Berkson was the one who coined the term &quot;logit&quot;, which is like the magic spell that powers logistic regression. ğŸª„âœ¨</p><blockquote><p>å¿«è¿›åˆ°20ä¸–çºªï¼Œçº¦ç‘Ÿå¤«Â·ä¼¯å…‹æ£®ï¼ˆJoseph Berksonï¼‰æ¥è¿‡äº†é€»è¾‘å›å½’çš„æ¥åŠ›æ£’ã€‚ä»–ä»1944å¹´å¼€å§‹ä½¿é€»è¾‘å›å½’ç°ä»£åŒ–ï¼Œä½¿å…¶æˆä¸ºç»Ÿè®¡é¢†åŸŸçš„ä¸»è¦å†…å®¹ã€‚ä¼¯å…‹æ£®æ˜¯â€œlogitâ€ä¸€è¯çš„åˆ›é€ è€…ï¼Œå®ƒå°±åƒæ˜¯æ¨åŠ¨é€»è¾‘å›å½’çš„é­”å’’ã€‚ğŸª„âœ¨</p></blockquote><p>Initially, logistic regression found its playground in the biological sciences, helping researchers make sense of binary outcomes like survival or demise of species based on various factors. However, it wasnâ€™t long before social scientists adopted this magical tool to predict categorical outcomes in their own fields. ğŸ§ªğŸ“Š</p><blockquote><p>æœ€åˆï¼Œé€»è¾‘å›å½’åœ¨ç”Ÿç‰©ç§‘å­¦ä¸­æ‰¾åˆ°äº†å®ƒçš„æ¸¸ä¹åœºï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ ¹æ®å„ç§å› ç´ ç†è§£äºŒå…ƒç»“æœï¼Œå¦‚ç‰©ç§çš„ç”Ÿå­˜æˆ–æ¶ˆäº¡ã€‚ç„¶è€Œï¼Œæ²¡è¿‡å¤šä¹…ï¼Œç¤¾ä¼šç§‘å­¦å®¶å°±é‡‡ç”¨äº†è¿™ç§ç¥å¥‡çš„å·¥å…·æ¥é¢„æµ‹ä»–ä»¬è‡ªå·±é¢†åŸŸçš„åˆ†ç±»ç»“æœã€‚ğŸ§ªğŸ“Š</p></blockquote><blockquote></blockquote><p>With its roots deeply embedded in history, logistic regression now serves as a bridge between the mathematical and the empirical, enabling us to navigate the binary landscapes of our data-driven world. ğŸŒ‰</p><blockquote><p>é€»è¾‘å›å½’æ·±æ·±æ¤æ ¹äºå†å²ï¼Œç°åœ¨æˆä¸ºæ•°å­¦å’Œå®è¯ä¹‹é—´çš„æ¡¥æ¢ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿé©¾é©­æ•°æ®é©±åŠ¨ä¸–ç•Œçš„äºŒå…ƒæ™¯è§‚ã€‚ğŸŒ‰</p></blockquote><p>Now that we&#39;ve skimmed the surface of its rich history, are you ready to dive into the mechanism that drives logistic regression? ğŸ¤¿</p><blockquote><p>ç°åœ¨æˆ‘ä»¬å·²ç»ç•¥è¿‡äº†å…¶ä¸°å¯Œå†å²çš„è¡¨é¢ï¼Œæ‚¨å‡†å¤‡å¥½æ·±å…¥äº†è§£é©±åŠ¨é€»è¾‘å›å½’çš„æœºåˆ¶äº†å—ï¼ŸğŸ¤¿</p></blockquote><h2 id="how-it-works-å·¥ä½œåŸç†" tabindex="-1">How it Works å·¥ä½œåŸç† <a class="header-anchor" href="#how-it-works-å·¥ä½œåŸç†" aria-label="Permalink to &quot;How it Works å·¥ä½œåŸç†&quot;">â€‹</a></h2><p>Logistic regression is like that friendly guide that helps us trek through the binary jungles of data. At its core, it&#39;s a statistical model used to estimate the probability of a binary outcome based on one or more independent variables. ğŸ²ğŸŒ¿</p><blockquote><p>é€»è¾‘å›å½’å°±åƒé‚£ä¸ªå‹å¥½çš„æŒ‡å—ï¼Œå¸®åŠ©æˆ‘ä»¬åœ¨æ•°æ®çš„äºŒå…ƒä¸›æ—ä¸­è·‹æ¶‰ã€‚å®ƒçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªç»Ÿè®¡æ¨¡å‹ï¼Œç”¨äºæ ¹æ®ä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡ä¼°è®¡äºŒå…ƒç»“æœçš„æ¦‚ç‡ã€‚ğŸ²ğŸŒ¿</p></blockquote><p>Logistic regression estimates the probability of an event occurring (like casting a vote or identifying a spam email) based on a dataset of independent variables. Unlike linear regression, which predicts a continuous outcome, logistic regression predicts the probability of a discrete outcome, which is mapped to a binary value (0 or 1, Yes or No). The beauty of logistic regression lies in its simplicity and the way it bounds the outcome between 0 and 1, thanks to the logistic function (also known as the sigmoid function):</p><blockquote><p>é€»è¾‘å›å½’æ ¹æ®è‡ªå˜é‡æ•°æ®é›†ä¼°è®¡äº‹ä»¶å‘ç”Ÿï¼ˆå¦‚æŠ•ç¥¨æˆ–è¯†åˆ«åƒåœ¾é‚®ä»¶ï¼‰çš„æ¦‚ç‡ã€‚ä¸é¢„æµ‹è¿ç»­ç»“æœçš„çº¿æ€§å›å½’ä¸åŒï¼Œé€»è¾‘å›å½’é¢„æµ‹ç¦»æ•£ç»“æœçš„æ¦‚ç‡ï¼Œè¯¥ç»“æœæ˜ å°„åˆ°äºŒè¿›åˆ¶å€¼ï¼ˆ0 æˆ– 1ï¼Œæ˜¯æˆ–å¦ï¼‰ã€‚é€»è¾‘å›å½’çš„ç¾å¦™ä¹‹å¤„åœ¨äºå®ƒçš„ç®€å•æ€§ä»¥åŠå®ƒå°†ç»“æœé™åˆ¶åœ¨ 0 å’Œ 1 ä¹‹é—´çš„æ–¹å¼ï¼Œè¿™è¦å½’åŠŸäºé€»è¾‘å‡½æ•°ï¼ˆä¹Ÿç§°ä¸º sigmoid å‡½æ•°ï¼‰ï¼š</p></blockquote><p><img src="'+a+'" alt="Alt text"></p><p>Here, è¿™é‡Œ</p><ul><li>P(Y=1) is the probability of the binary outcome being 1.</li></ul><blockquote><ul><li>Pï¼ˆY=1ï¼‰ æ˜¯äºŒå…ƒç»“æœä¸º 1 çš„æ¦‚ç‡ã€‚</li></ul></blockquote><ul><li>beta_0, beta_1, ldots, beta_n are the coefficients that need to be estimated from the data.</li></ul><blockquote><ul><li>beta_0ã€beta_1ã€ldots beta_n æ˜¯éœ€è¦ä»æ•°æ®ä¸­ä¼°è®¡çš„ç³»æ•°ã€‚</li></ul></blockquote><ul><li>X_1, X_n are the independent variables.</li></ul><blockquote><ul><li>X_1ï¼ŒX_næ˜¯è‡ªå˜é‡ã€‚</li></ul></blockquote><p>Imagine you&#39;re at a game show, and based on certain characteristics (like your age, the number of game shows you&#39;ve attended before, and the color of shirt you&#39;re wearing), the host is trying to predict whether you&#39;ll choose Door #1 or Door #2. Logistic regression is like the host&#39;s educated guessing game, where the host evaluates the likelihood of you choosing Door #1 based on the characteristics you exhibit. ğŸšªğŸ¤”</p><blockquote><p>æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£åœ¨å‚åŠ ä¸€ä¸ªæ¸¸æˆèŠ‚ç›®ï¼Œæ ¹æ®æŸäº›ç‰¹å¾ï¼ˆæ¯”å¦‚ä½ çš„å¹´é¾„ã€ä½ ä»¥å‰å‚åŠ çš„æ¸¸æˆèŠ‚ç›®çš„æ•°é‡ä»¥åŠä½ ç©¿çš„è¡¬è¡«çš„é¢œè‰²ï¼‰ï¼Œä¸»æŒäººè¯•å›¾é¢„æµ‹ä½ ä¼šé€‰æ‹©é—¨ #1 è¿˜æ˜¯é—¨ #2ã€‚é€»è¾‘å›å½’å°±åƒä¸»æŒäººå—è¿‡æ•™è‚²çš„çŒœè°œæ¸¸æˆï¼Œä¸»æŒäººæ ¹æ®ä½ è¡¨ç°å‡ºçš„ç‰¹å¾æ¥è¯„ä¼°ä½ é€‰æ‹©é—¨ #1 çš„å¯èƒ½æ€§ã€‚ğŸšªğŸ¤”</p></blockquote><h2 id="the-algorithm-ç®—æ³•" tabindex="-1">The Algorithm ç®—æ³• <a class="header-anchor" href="#the-algorithm-ç®—æ³•" aria-label="Permalink to &quot;The Algorithm ç®—æ³•&quot;">â€‹</a></h2><p>Venturing into the algorithmic heart of logistic regression is like understanding the recipe that cooks up our binary predictions. ğŸ² Let&#39;s dissect the steps in a simplistic manner:</p><blockquote><p>æ¶‰è¶³é€»è¾‘å›å½’çš„ç®—æ³•æ ¸å¿ƒï¼Œå°±åƒç†è§£äº†æˆ‘ä»¬äºŒå…ƒé¢„æµ‹çš„é…æ–¹ä¸€æ ·ã€‚ğŸ² è®©æˆ‘ä»¬ä»¥ç®€å•çš„æ–¹å¼å‰–æè¿™äº›æ­¥éª¤ï¼š</p></blockquote><ol><li><strong>Collection of Data</strong>: Gather the data that holds the features (independent variables) and the target variable (the binary outcome we want to predict).</li></ol><blockquote><ol><li><strong>æ•°æ®æ”¶é›†</strong>ï¼šæ”¶é›†åŒ…å«ç‰¹å¾ï¼ˆè‡ªå˜é‡ï¼‰å’Œç›®æ ‡å˜é‡ï¼ˆæˆ‘ä»¬è¦é¢„æµ‹çš„äºŒå…ƒç»“æœï¼‰çš„æ•°æ®ã€‚</li></ol></blockquote><ol start="2"><li><strong>Initialization</strong>: Set initial values for the coefficients beta_0, beta_1, ldots, beta_n.</li></ol><blockquote><ol start="2"><li><strong>åˆå§‹åŒ–</strong>ï¼šè®¾ç½®ç³»æ•° beta_0ã€beta_1ã€ldots beta_n çš„åˆå§‹å€¼ã€‚</li></ol></blockquote><ol start="3"><li><strong>Calculation of Prediction</strong>: Using the logistic (sigmoid) function, calculate the probability of the binary outcome being 1 for each observation in the data:</li></ol><blockquote><ol start="3"><li><strong>é¢„æµ‹çš„è®¡ç®—</strong>ï¼šä½¿ç”¨é€»è¾‘ï¼ˆsigmoidï¼‰å‡½æ•°ï¼Œè®¡ç®—æ•°æ®ä¸­æ¯ä¸ªè§‚æµ‹å€¼çš„äºŒå…ƒç»“æœä¸º1çš„æ¦‚ç‡ï¼š <img src="'+s+'" alt="Alt text"></li></ol></blockquote><ol start="4"><li><strong>Evaluation of Log-Likelihood</strong>: Compute the log-likelihood of observing the given set of outcomes with the current coefficients.</li></ol><blockquote><ol start="4"><li><strong>å¯¹æ•°ä¼¼ç„¶è¯„ä¼°</strong>ï¼šè®¡ç®—ä½¿ç”¨å½“å‰ç³»æ•°è§‚å¯Ÿç»™å®šç»“æœé›†çš„å¯¹æ•°ä¼¼ç„¶ã€‚</li></ol></blockquote><ol start="5"><li><strong>Update Coefficients</strong>: Update the coefficients to maximize the log-likelihood using an optimization technique like Gradient Descent.</li></ol><blockquote><ol start="5"><li><strong>æ›´æ–°ç³»æ•°</strong>ï¼šä½¿ç”¨æ¢¯åº¦ä¸‹é™ç­‰ä¼˜åŒ–æŠ€æœ¯æ›´æ–°ç³»æ•°ä»¥æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ã€‚</li></ol></blockquote><ol start="6"><li><strong>Convergence Check</strong>: Check if the coefficients have converged (i.e., the changes in the coefficients are negligible), or if the maximum number of iterations has been reached.</li></ol><blockquote><ol start="6"><li><strong>æ”¶æ•›æ€§æ£€æŸ¥</strong>ï¼šæ£€æŸ¥ç³»æ•°æ˜¯å¦æ”¶æ•›ï¼ˆå³ç³»æ•°çš„å˜åŒ–å¯ä»¥å¿½ç•¥ä¸è®¡ï¼‰ï¼Œæˆ–è€…æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚</li></ol></blockquote><ol start="7"><li><strong>Model Evaluation</strong>: Evaluate the performance of the logistic regression model using appropriate metrics like accuracy, precision, recall, etc.</li></ol><blockquote><ol start="7"><li><strong>æ¨¡å‹è¯„ä¼°</strong>ï¼šä½¿ç”¨é€‚å½“çš„æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡ç­‰ï¼‰è¯„ä¼°é€»è¾‘å›å½’æ¨¡å‹çš„æ€§èƒ½ã€‚</li></ol></blockquote><h3 id="invasion-of-the-spam-marauders" tabindex="-1">Invasion of the Spam Marauders <a class="header-anchor" href="#invasion-of-the-spam-marauders" aria-label="Permalink to &quot;Invasion of the Spam Marauders&quot;">â€‹</a></h3><blockquote><h3 id="åƒåœ¾é‚®ä»¶æ å¤ºè€…çš„å…¥ä¾µ" tabindex="-1">åƒåœ¾é‚®ä»¶æ å¤ºè€…çš„å…¥ä¾µ <a class="header-anchor" href="#åƒåœ¾é‚®ä»¶æ å¤ºè€…çš„å…¥ä¾µ" aria-label="Permalink to &quot;åƒåœ¾é‚®ä»¶æ å¤ºè€…çš„å…¥ä¾µ&quot;">â€‹</a></h3></blockquote><p>Once upon a time in the land of Inboxia, there lived a diligent gatekeeper named Logi. Logi had a very important jobâ€”to guard the gates of the grand Email Palace against the invasion of Spam Marauders. The Marauders were notorious for crashing the peaceful gatherings of the genuine Email Folks and causing havoc. ğŸ°ğŸ›¡ï¸</p><blockquote><p>å¾ˆä¹…å¾ˆä¹…ä»¥å‰ï¼Œåœ¨Inboxiaçš„åœŸåœ°ä¸Šï¼Œä½ç€ä¸€ä½åå«Logiçš„å‹¤å¥‹çš„å®ˆé—¨äººã€‚Logi æœ‰ä¸€é¡¹éå¸¸é‡è¦çš„å·¥ä½œâ€”â€”å®ˆå«å®ä¼Ÿçš„ç”µå­é‚®ä»¶å®«æ®¿çš„å¤§é—¨ï¼ŒæŠµå¾¡åƒåœ¾é‚®ä»¶æ å¤ºè€…çš„å…¥ä¾µã€‚æ å¤ºè€…å› ç ´åçœŸæ­£çš„ç”µå­é‚®ä»¶äººæ°‘çš„å’Œå¹³èšä¼šå¹¶é€ æˆç ´åè€Œè‡­åæ˜­è‘—ã€‚ğŸ°ğŸ›¡ï¸</p></blockquote><p>Logi had a magic scroll named Logistic Regression, bestowed upon by the ancient Statisticians. The scroll had the power to unveil the guise of the Spam Marauders based on certain traits they exhibited. Two traits were particularly tellingâ€”their flashy Armor of Capital Letters and the deceptive Links of Deception they carried. ğŸ“œâœ¨</p><blockquote><p>Logi æœ‰ä¸€ä¸ªåä¸º Logistic Regression çš„é­”æ³•å·è½´ï¼Œç”±å¤ä»£ç»Ÿè®¡å­¦å®¶èµ‹äºˆã€‚è¯¥å·è½´æœ‰èƒ½åŠ›æ ¹æ®åƒåœ¾é‚®ä»¶æ å¤ºè€…è¡¨ç°å‡ºçš„æŸäº›ç‰¹å¾æ­å¼€ä»–ä»¬çš„ä¼ªè£…ã€‚æœ‰ä¸¤ä¸ªç‰¹å¾ç‰¹åˆ«èƒ½è¯´æ˜é—®é¢˜â€”â€”ä»–ä»¬åä¸½çš„å¤§å†™å­—æ¯ç›”ç”²å’Œä»–ä»¬æºå¸¦çš„æ¬ºéª—æ€§é“¾æ¥ã€‚ğŸ“œâœ¨</p></blockquote><h3 id="chapter-1-gathering-the-clues" tabindex="-1">Chapter 1: Gathering the Clues <a class="header-anchor" href="#chapter-1-gathering-the-clues" aria-label="Permalink to &quot;Chapter 1: Gathering the Clues&quot;">â€‹</a></h3><blockquote><h3 id="ç¬¬-1-ç« -æ”¶é›†çº¿ç´¢" tabindex="-1">ç¬¬ 1 ç« ï¼šæ”¶é›†çº¿ç´¢ <a class="header-anchor" href="#ç¬¬-1-ç« -æ”¶é›†çº¿ç´¢" aria-label="Permalink to &quot;ç¬¬ 1 ç« ï¼šæ”¶é›†çº¿ç´¢&quot;">â€‹</a></h3></blockquote><p>Before the sun rose every day, Logi would gather all the messages waiting at the gates. Each message carried with it the frequency of flashy armor (capital letters) and whether it bore any Links of Deception. These were recorded as (X_1) and (X_2) in the magic scroll.</p><blockquote><p>æ¯å¤©å¤ªé˜³å‡èµ·ä¹‹å‰ï¼ŒLogi éƒ½ä¼šæ”¶é›†æ‰€æœ‰åœ¨é—¨å£ç­‰å¾…çš„ä¿¡æ¯ã€‚æ¯æ¡ä¿¡æ¯éƒ½å¸¦æœ‰åä¸½ç›”ç”²ï¼ˆå¤§å†™å­—æ¯ï¼‰çš„é¢‘ç‡ä»¥åŠå®ƒæ˜¯å¦å¸¦æœ‰ä»»ä½•æ¬ºéª—é“¾æ¥ã€‚è¿™äº›åœ¨é­”æ³•å·è½´ä¸­è¢«è®°å½•ä¸º \\ï¼ˆX_1\\ï¼‰ å’Œ \\ï¼ˆX_2\\ï¼‰ã€‚</p></blockquote><h3 id="chapter-2-invoking-the-magic-scroll" tabindex="-1">Chapter 2: Invoking the Magic Scroll <a class="header-anchor" href="#chapter-2-invoking-the-magic-scroll" aria-label="Permalink to &quot;Chapter 2: Invoking the Magic Scroll&quot;">â€‹</a></h3><blockquote><h3 id="ç¬¬-2-ç« -å¬å”¤é­”æ³•å·è½´" tabindex="-1">ç¬¬ 2 ç« ï¼šå¬å”¤é­”æ³•å·è½´ <a class="header-anchor" href="#ç¬¬-2-ç« -å¬å”¤é­”æ³•å·è½´" aria-label="Permalink to &quot;ç¬¬ 2 ç« ï¼šå¬å”¤é­”æ³•å·è½´&quot;">â€‹</a></h3></blockquote><p>As the dawn broke, Logi would invoke the magic scroll to estimate the probability of each message being a Spam Marauder. The formula whispered by the scroll was:</p><blockquote><p>å½“é»æ˜ç ´æ™“æ—¶ï¼ŒLogi ä¼šè°ƒç”¨é­”æ³•å·è½´æ¥ä¼°è®¡æ¯å°é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶æ å¤ºè€…çš„æ¦‚ç‡ã€‚å·è½´ä½å£°è¯´å‡ºçš„å…¬å¼æ˜¯ï¼š</p></blockquote><p>Here, è¿™é‡Œ</p><blockquote><p>-P(Y=1) was the probability of a message being a Spam Marauder. -Pï¼ˆY=1ï¼‰ æ˜¯é‚®ä»¶æˆä¸ºåƒåœ¾é‚®ä»¶æ å¤ºè€…çš„æ¦‚ç‡ã€‚</p></blockquote><ul><li>beta_0, beta_1, beta_2 were the mystical coefficients that the scroll would learn from the data.</li></ul><blockquote><ul><li>beta_0ï¼Œbeta_1ï¼Œbeta_2æ˜¯æ»šåŠ¨ä»æ•°æ®ä¸­å­¦ä¹ çš„ç¥ç§˜ç³»æ•°ã€‚</li></ul></blockquote><h3 id="chapter-3-learning-from-the-mystical-coefficients" tabindex="-1">Chapter 3: Learning from the Mystical Coefficients <a class="header-anchor" href="#chapter-3-learning-from-the-mystical-coefficients" aria-label="Permalink to &quot;Chapter 3: Learning from the Mystical Coefficients&quot;">â€‹</a></h3><blockquote><h3 id="ç¬¬-3-ç« -ä»ç¥ç§˜ç³»æ•°ä¸­å­¦ä¹ " tabindex="-1">ç¬¬ 3 ç« ï¼šä»ç¥ç§˜ç³»æ•°ä¸­å­¦ä¹  <a class="header-anchor" href="#ç¬¬-3-ç« -ä»ç¥ç§˜ç³»æ•°ä¸­å­¦ä¹ " aria-label="Permalink to &quot;ç¬¬ 3 ç« ï¼šä»ç¥ç§˜ç³»æ•°ä¸­å­¦ä¹ &quot;">â€‹</a></h3></blockquote><p>The magic scroll was wise. It would adjust the mystical coefficients to learn from the messages. The scroll wanted to maximize the likelihood of correctly identifying the Spam Marauders. This quest led to a dance of mathematicsâ€”the Gradient Descentâ€”where the scroll iteratively adjusted the coefficients to find the best values.</p><blockquote><p>é­”æ³•å·è½´æ˜¯æ˜æ™ºçš„ã€‚å®ƒå°†è°ƒæ•´ç¥ç§˜ç³»æ•°ä»¥ä»æ¶ˆæ¯ä¸­å­¦ä¹ ã€‚è¯¥å·è½´å¸Œæœ›æœ€å¤§é™åº¦åœ°æé«˜æ­£ç¡®è¯†åˆ«åƒåœ¾é‚®ä»¶æ å¤ºè€…çš„å¯èƒ½æ€§ã€‚è¿™ç§æ¢ç´¢å¯¼è‡´äº†æ•°å­¦ä¹‹èˆâ€”â€”æ¢¯åº¦ä¸‹é™â€”â€”æ»šåŠ¨æ»šåŠ¨è¿­ä»£è°ƒæ•´ç³»æ•°ä»¥æ‰¾åˆ°æœ€ä½³å€¼ã€‚</p></blockquote><h3 id="chapter-4-the-verdict-of-the-scroll" tabindex="-1">Chapter 4: The Verdict of the Scroll <a class="header-anchor" href="#chapter-4-the-verdict-of-the-scroll" aria-label="Permalink to &quot;Chapter 4: The Verdict of the Scroll&quot;">â€‹</a></h3><blockquote><h3 id="ç¬¬-4-ç« -å·è½´çš„åˆ¤å†³" tabindex="-1">ç¬¬ 4 ç« ï¼šå·è½´çš„åˆ¤å†³ <a class="header-anchor" href="#ç¬¬-4-ç« -å·è½´çš„åˆ¤å†³" aria-label="Permalink to &quot;ç¬¬ 4 ç« ï¼šå·è½´çš„åˆ¤å†³&quot;">â€‹</a></h3></blockquote><p>With the mystical coefficients finely tuned, the magic scroll would whisper to Logi the likelihood of each message being from the Spam Marauders. If the probability was high, the message was turned away from the gates, ensuring the peaceful gathering of Email Folks remained undisturbed.</p><blockquote><p>éšç€ç¥ç§˜ç³»æ•°çš„å¾®è°ƒï¼Œé­”æ³•å·è½´ä¼šå‘ç½—å‰ä½å£°è¯´å‡ºæ¯æ¡æ¶ˆæ¯éƒ½æ¥è‡ªåƒåœ¾é‚®ä»¶æ å¤ºè€…çš„å¯èƒ½æ€§ã€‚å¦‚æœå¯èƒ½æ€§å¾ˆé«˜ï¼Œåˆ™å°†é‚®ä»¶æ‹’ä¹‹é—¨å¤–ï¼Œç¡®ä¿ç”µå­é‚®ä»¶äººå‘˜çš„å’Œå¹³èšä¼šä¸å—å¹²æ‰°ã€‚</p></blockquote><p>Through days and nights, Logi and the magic scroll stood guard, ensuring the nefarious Spam Marauders were kept at bay, and the land of Inboxia remained a haven for genuine interactions. ğŸŒ…</p><blockquote><p>åœ¨æ—¥æ—¥å¤œå¤œé‡Œï¼ŒLogi å’Œé­”æ³•å·è½´å®ˆå«ç€ï¼Œç¡®ä¿é‚ªæ¶çš„åƒåœ¾é‚®ä»¶æ å¤ºè€…è¢«æ‹’ä¹‹é—¨å¤–ï¼Œè€Œ Inboxia çš„åœŸåœ°ä»ç„¶æ˜¯çœŸæ­£äº’åŠ¨çš„é¿é£æ¸¯ã€‚ğŸŒ…</p></blockquote><p>And thus, through the lens of a whimsical tale, we&#39;ve journeyed through the algorithmic essence of logistic regression in the realm of spam detection.</p><blockquote><p>å› æ­¤ï¼Œé€šè¿‡ä¸€ä¸ªå¼‚æƒ³å¤©å¼€çš„æ•…äº‹çš„é•œå¤´ï¼Œæˆ‘ä»¬å·²ç»äº†è§£äº†åƒåœ¾é‚®ä»¶æ£€æµ‹é¢†åŸŸé€»è¾‘å›å½’çš„ç®—æ³•æœ¬è´¨ã€‚</p></blockquote><h2 id="advantages-ä¼˜ç‚¹" tabindex="-1">Advantages ä¼˜ç‚¹ <a class="header-anchor" href="#advantages-ä¼˜ç‚¹" aria-label="Permalink to &quot;Advantages ä¼˜ç‚¹&quot;">â€‹</a></h2><p>In the enchanted kingdom of Data Science, Logistic Regression is hailed as a valiant knight ğŸ›¡ï¸. Here are some virtues that make it a favorite amongst the kingdom&#39;s scholars:</p><blockquote><p>åœ¨æ•°æ®ç§‘å­¦çš„é­”æ³•ç‹å›½ä¸­ï¼Œé€»è¾‘å›å½’è¢«èª‰ä¸ºè‹±å‹‡çš„éª‘å£«ğŸ›¡ï¸ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä½¿å®ƒæˆä¸ºç‹å›½å­¦è€…æœ€çˆ±çš„ç¾å¾·ï¼š</p></blockquote><ol><li><strong>Simplicity</strong>: Logistic Regression is like a clear crystal ball ğŸ”®â€”easy to interpret and fathom. Its essence is not shrouded in enigma, making it a friendly companion on many quests.</li></ol><blockquote><ol><li><strong>ç®€å•æ€§</strong>ï¼šé€»è¾‘å›å½’å°±åƒä¸€ä¸ªé€æ˜çš„æ°´æ™¶çƒğŸ”®â€”â€”æ˜“äºè§£é‡Šå’Œç†è§£ã€‚å®ƒçš„æœ¬è´¨å¹¶ä¸ç¬¼ç½©åœ¨è°œå›¢ä¸­ï¼Œä½¿å…¶æˆä¸ºè®¸å¤šä»»åŠ¡çš„å‹å¥½ä¼´ä¾£ã€‚</li></ol></blockquote><ol start="2"><li><strong>Efficiency</strong>: Itâ€™s a swift steed ğŸ on the computational battleground. Logistic Regression hastens through training with the grace and speed of a coursing river, saving precious time in the ticking hourglass â³.</li></ol><blockquote><ol start="2"><li><strong>æ•ˆç‡</strong>ï¼šå®ƒæ˜¯è®¡ç®—æˆ˜åœºä¸Šçš„ä¸€åŒ¹æ•æ·çš„éªé©¬ğŸã€‚åå‹¤å›å½’ä»¥æ²³æµçš„ä¼˜é›…å’Œé€Ÿåº¦åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œåœ¨æ»´ç­”ä½œå“çš„æ²™æ¼â³ä¸­èŠ‚çœå®è´µçš„æ—¶é—´ã€‚</li></ol></blockquote><ol start="3"><li><strong>Proclivity for Binary Battles</strong>: It thrives in the lands of binary outcomes ğŸ”„. When the battle cry is between â€˜Yesâ€™ and â€˜Noâ€™, Logistic Regression is the chosen champion.</li></ol><blockquote><ol start="3"><li><strong>äºŒå…ƒæˆ˜æ–—çš„å€¾å‘</strong>ï¼šå®ƒåœ¨äºŒå…ƒç»“æœğŸ”„çš„åœŸåœ°ä¸ŠèŒå£®æˆé•¿ã€‚å½“æˆ˜æ–—å£å·ä»‹äºâ€œæ˜¯â€å’Œâ€œå¦â€ä¹‹é—´æ—¶ï¼Œé€»è¾‘å›å½’æ˜¯è¢«é€‰ä¸­çš„å† å†›ã€‚</li></ol></blockquote><ol start="4"><li><strong>Resistance to Overfitting</strong>: With noble allies like regularization, Logistic Regression stands resilient against the trickster curse of overfitting, ensuring the model doesnâ€™t get entranced by the whispers of noisy data ğŸ­.</li></ol><blockquote><ol start="4"><li><strong>æŠ—è¿‡æ‹Ÿåˆ</strong>ï¼šæœ‰äº†æ­£åˆ™åŒ–è¿™æ ·çš„é«˜è´µç›Ÿå‹ï¼ŒLogistic Regression å¯ä»¥æŠµå¾¡è¿‡æ‹Ÿåˆçš„éª—å­è¯…å’’ï¼Œç¡®ä¿æ¨¡å‹ä¸ä¼šè¢«å˜ˆæ‚çš„æ•°æ®ğŸ­çš„çªƒçªƒç§è¯­æ‰€å¸å¼•ã€‚</li></ol></blockquote><h2 id="disadvantages-ç¼ºç‚¹" tabindex="-1">Disadvantages ç¼ºç‚¹ <a class="header-anchor" href="#disadvantages-ç¼ºç‚¹" aria-label="Permalink to &quot;Disadvantages ç¼ºç‚¹&quot;">â€‹</a></h2><p>Yet, every knight has its Achilles&#39; heel. Here are the trials that Logistic Regression faces:</p><blockquote><p>ç„¶è€Œï¼Œæ¯ä¸ªéª‘å£«éƒ½æœ‰å…¶è‡´å‘½å¼±ç‚¹ã€‚ä»¥ä¸‹æ˜¯é€»è¾‘å›å½’é¢ä¸´çš„è€ƒéªŒï¼š</p></blockquote><ol><li><strong>Curse of Linearity</strong>: It lives under the spell of linearity ğŸ“, assuming a straight-line relationship between the independent variables and the log odds of the dependent variable. This spell binds Logistic Regression when the real-world data desires to dance in the wild rhythm of non-linearity.</li></ol><blockquote><ol><li><strong>çº¿æ€§çš„è¯…å’’</strong>ï¼šå®ƒå­˜åœ¨äºçº¿æ€§ğŸ“çš„é­”å’’ä¸‹ï¼Œå‡è®¾è‡ªå˜é‡ä¸å› å˜é‡çš„å¯¹æ•°å‡ ç‡ä¹‹é—´å­˜åœ¨ç›´çº¿å…³ç³»ã€‚å½“çœŸå®ä¸–ç•Œçš„æ•°æ®å¸Œæœ›åœ¨éçº¿æ€§çš„ç‹‚é‡èŠ‚å¥ä¸­è·³èˆæ—¶ï¼Œè¿™ä¸ªå’’è¯­ä¼šç»‘å®šé€»è¾‘å›å½’ã€‚</li></ol></blockquote><ol start="2"><li><strong>Struggles with Many Features</strong>: In the garden of numerous features, our knight may find itself entangled amidst thorns ğŸŒ¹. If the observations are fewer than the features, Logistic Regression might succumb to overfittingâ€™s deceit.</li></ol><blockquote><ol start="2"><li><strong>ä¸è®¸å¤šç‰¹å¾çš„æ–—äº‰</strong>ï¼šåœ¨ä¼—å¤šç‰¹å¾çš„èŠ±å›­ä¸­ï¼Œæˆ‘ä»¬çš„éª‘å£«å¯èƒ½ä¼šå‘ç°è‡ªå·±è¢«è†æ£˜ğŸŒ¹ç¼ ä½äº†ã€‚å¦‚æœè§‚æµ‹å€¼å°äºç‰¹å¾ï¼Œåˆ™é€»è¾‘å›å½’å¯èƒ½ä¼šå±ˆæœäºè¿‡æ‹Ÿåˆçš„æ¬ºéª—ã€‚</li></ol></blockquote><ol start="3"><li><strong>Binary Vision</strong>: Its gaze is fixed on binary horizons ğŸŒ…. When the quest involves multiclass classification, Logistic Regression requires the fellowship of One-vs-Rest to battle valiantly.</li></ol><blockquote><p>3.<strong>äºŒå…ƒè§†è§‰</strong>ï¼šå®ƒçš„ç›®å…‰å›ºå®šåœ¨äºŒå…ƒè§†ç•ŒğŸŒ…ä¸Šã€‚å½“ä»»åŠ¡æ¶‰åŠå¤šèŒä¸šåˆ†ç±»æ—¶ï¼Œé€»è¾‘å›å½’éœ€è¦ä¸€å¯¹ä¸€ä¸ä¼‘æ¯çš„å›¢å¥‘æ‰èƒ½è‹±å‹‡æˆ˜æ–—ã€‚</p></blockquote><h2 id="applications-åº”ç”¨" tabindex="-1">Applications åº”ç”¨ <a class="header-anchor" href="#applications-åº”ç”¨" aria-label="Permalink to &quot;Applications åº”ç”¨&quot;">â€‹</a></h2><p>Armed with the sword of binary classification, Logistic Regression has championed many a cause in the real world:</p><blockquote><p>åœ¨äºŒå…ƒåˆ†ç±»çš„å‰‘ä¸‹ï¼Œé€»è¾‘å›å½’åœ¨ç°å®ä¸–ç•Œä¸­æ”¯æŒäº†è®¸å¤šäº‹ä¸šï¼š</p></blockquote><ol><li><strong>Spam Detection</strong>: As narrated in our whimsical tale, Logistic Regression is a vigilant guard against Spam Marauders, ensuring peace in the land of Inboxia ğŸ’Œ.</li></ol><blockquote><ol><li><strong>åƒåœ¾é‚®ä»¶æ£€æµ‹</strong>ï¼šæ­£å¦‚æˆ‘ä»¬å¼‚æƒ³å¤©å¼€çš„æ•…äº‹ä¸­æ‰€å™è¿°çš„é‚£æ ·ï¼Œé€»è¾‘å›å½’æ˜¯å¯¹åƒåœ¾é‚®ä»¶æ å¤ºè€…çš„è­¦æƒ•å®ˆå«ï¼Œç¡®ä¿ Inboxia ğŸ’Œ åœŸåœ°çš„å’Œå¹³ã€‚</li></ol></blockquote><ol start="2"><li><strong>Credit Approval</strong>: In the bustling markets of finance, Logistic Regression is the discerning sage that predicts who is worthy of credit approval ğŸ’³.</li></ol><blockquote><ol start="2"><li><strong>ä¿¡ç”¨å®¡æ‰¹</strong>ï¼šåœ¨ç†™ç†™æ”˜æ”˜çš„é‡‘èå¸‚åœºä¸­ï¼ŒLogistic Regression æ˜¯é¢„æµ‹è°å€¼å¾—æˆä¿¡å®¡ğŸ’³æ‰¹çš„æ…§çœ¼æ™ºè€…ã€‚</li></ol></blockquote><ol start="3"><li><strong>Medical Diagnosis</strong>: In the hallowed halls of healing, Logistic Regression aids in deciphering the runes of disease diagnosis and patient outcome prediction ğŸ©º.</li></ol><blockquote><ol start="3"><li><strong>åŒ»å­¦è¯Šæ–­</strong>ï¼šåœ¨ç¥åœ£çš„ç–—æ„ˆæ®¿å ‚ä¸­ï¼Œé€»è¾‘å›å½’æœ‰åŠ©äºç ´è¯‘ç–¾ç—…è¯Šæ–­å’Œæ‚£è€…é¢„åé¢„æµ‹ğŸ©ºçš„ç¬¦æ–‡ã€‚</li></ol></blockquote><ol start="4"><li><strong>Customer Churn Prediction</strong>: Amidst the lively market squares, it lends its foresight in distinguishing the loyal patrons from the fleeting ones, aiding the merchants in nurturing lasting bonds ğŸ¤.</li></ol><blockquote><ol start="4"><li><strong>å®¢æˆ·æµå¤±é¢„æµ‹</strong>ï¼šåœ¨çƒ­é—¹çš„é›†å¸‚å¹¿åœºä¸­ï¼Œå®ƒå…·æœ‰è¿œè§å“è¯†ï¼Œå¯ä»¥åŒºåˆ†å¿ å®çš„é¡¾å®¢å’Œè½¬ç¬å³é€çš„é¡¾å®¢ï¼Œå¸®åŠ©å•†å®¶å»ºç«‹æŒä¹…çš„è”ç³»ğŸ¤ã€‚</li></ol></blockquote><h2 id="tl-dr" tabindex="-1">TL;DR <a class="header-anchor" href="#tl-dr" aria-label="Permalink to &quot;TL;DR&quot;">â€‹</a></h2><p>In the whimsical kingdom of Data Science, Logistic Regression emerges as a valiant knight, guarding the realms of binary classification with honor. Its sword of simplicity and shield of efficiency make it a beloved champion. Yet, the knight faces trials with the Curse of Linearity and the entangling garden of numerous features. Despite these challenges, Logistic Regression valiantly battles in real-world quests, from keeping the nefarious Spam Marauders at bay in the peaceful land of Inboxia, to aiding the discerning sages in finance and the healing seers in healthcare. Our knightâ€™s tale is an ode to the enduring legacy of logistic regression in the ever-evolving landscape of data science. ğŸ›¡ï¸âš”ï¸ğŸ‡</p><blockquote><p>åœ¨å¼‚æƒ³å¤©å¼€çš„æ•°æ®ç§‘å­¦ç‹å›½ä¸­ï¼ŒLogistic Regression ä»¥è‹±å‹‡çš„éª‘å£«èº«ä»½å‡ºç°ï¼Œå…‰è£åœ°å®ˆå«ç€äºŒå…ƒåˆ†ç±»é¢†åŸŸã€‚å®ƒçš„ç®€å•ä¹‹å‰‘å’Œæ•ˆç‡ä¹‹ç›¾ä½¿å®ƒæˆä¸ºæ·±å—å–œçˆ±çš„å† å†›ã€‚ç„¶è€Œï¼Œéª‘å£«é¢ä¸´ç€çº¿æ€§è¯…å’’å’Œä¼—å¤šç‰¹å¾çš„çº ç¼ èŠ±å›­çš„è€ƒéªŒã€‚å°½ç®¡é¢ä¸´è¿™äº›æŒ‘æˆ˜ï¼ŒLogistic Regression åœ¨ç°å®ä¸–ç•Œçš„ä»»åŠ¡ä¸­ä»ç„¶è‹±å‹‡ä½œæˆ˜ï¼Œä»åœ¨å’Œå¹³çš„ Inboxia åœŸåœ°ä¸Šé˜»æ­¢é‚ªæ¶çš„åƒåœ¾é‚®ä»¶æ å¤ºè€…ï¼Œåˆ°å¸®åŠ©é‡‘èé¢†åŸŸçš„æ•é”åœ£äººå’ŒåŒ»ç–—ä¿å¥é¢†åŸŸçš„æ²»æ„ˆå…ˆçŸ¥ã€‚æˆ‘ä»¬çš„éª‘å£«æ•…äº‹æ˜¯å¯¹æ•°æ®ç§‘å­¦ä¸æ–­å‘å±•çš„é¢†åŸŸä¸­é€»è¾‘å›å½’çš„æŒä¹…é—äº§çš„é¢‚æ­Œã€‚ğŸ›¡ï¸âš”ï¸ğŸ‡</p></blockquote><h2 id="vocabulary-list-è¯æ±‡è¡¨" tabindex="-1">Vocabulary List è¯æ±‡è¡¨ <a class="header-anchor" href="#vocabulary-list-è¯æ±‡è¡¨" aria-label="Permalink to &quot;Vocabulary List è¯æ±‡è¡¨&quot;">â€‹</a></h2><ul><li><strong>Logistic Regression</strong>: A statistical model used for binary classification, estimating the probability of an event occurrence based on one or more independent variables.</li></ul><blockquote><ul><li><strong>Logistic Regression</strong>ï¼šç”¨äºäºŒå…ƒåˆ†ç±»çš„ç»Ÿè®¡æ¨¡å‹ï¼ŒåŸºäºä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡ä¼°è®¡äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚</li></ul></blockquote><ul><li><strong>Binary Classification</strong>: The task of classifying the elements of a given set into two groups based on a classification rule.</li></ul><blockquote><ul><li><strong>äºŒå…ƒåˆ†ç±»</strong>ï¼šæ ¹æ®åˆ†ç±»è§„åˆ™å°†ç»™å®šé›†åˆçš„å…ƒç´ åˆ†ç±»ä¸ºä¸¤ç»„çš„ä»»åŠ¡ã€‚</li></ul></blockquote><ul><li><strong>Logit</strong>: The function used in logistic regression to squeeze the probability estimates between 0 and 1.</li></ul><blockquote><ul><li><strong>Logit</strong>ï¼šé€»è¾‘å›å½’ä¸­ä½¿ç”¨çš„å‡½æ•°ï¼Œç”¨äºå°†æ¦‚ç‡ä¼°è®¡å€¼å‹ç¼©åˆ° 0 åˆ° 1 ä¹‹é—´ã€‚</li></ul></blockquote><ul><li><strong>Gradient Descent</strong>: An optimization algorithm used to minimize some function by iteratively moving in the direction of steepest decrease.</li></ul><blockquote><ul><li><strong>æ¢¯åº¦ä¸‹é™</strong>ï¼šä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºé€šè¿‡å‘æœ€é™¡å³­çš„ä¸‹é™æ–¹å‘è¿­ä»£ç§»åŠ¨æ¥æœ€å°åŒ–æŸäº›åŠŸèƒ½ã€‚</li></ul></blockquote><ul><li><strong>Regularization</strong>: A technique used to prevent overfitting by adding a penalty term to the loss function.</li></ul><blockquote><ul><li><strong>æ­£åˆ™åŒ–</strong>ï¼šä¸€ç§é€šè¿‡å‘æŸå¤±å‡½æ•°æ·»åŠ æƒ©ç½šé¡¹æ¥é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ã€‚</li></ul></blockquote><ul><li><strong>Overfitting</strong>: A modeling error that occurs when a function is too closely aligned to a limited set of data points.</li></ul><blockquote><ul><li><strong>è¿‡æ‹Ÿåˆ</strong>ï¼šå½“å‡½æ•°ä¸ä¸€ç»„æœ‰é™çš„æ•°æ®ç‚¹è¿‡äºç´§å¯†å¯¹é½æ—¶å‘ç”Ÿçš„å»ºæ¨¡é”™è¯¯ã€‚</li></ul></blockquote><ul><li><strong>Multiclass Classification</strong>: The task of classifying the elements of a given set into more than two groups.</li></ul><blockquote><ul><li><strong>å¤šç±»åˆ†ç±»</strong>ï¼šå°†ç»™å®šé›†åˆçš„å…ƒç´ åˆ†ç±»ä¸ºä¸¤ç»„ä»¥ä¸Šçš„ä»»åŠ¡ã€‚</li></ul></blockquote>',127),n=[r];function c(h,g,u,d,b,p){return t(),o("div",null,n)}const k=e(l,[["render",c]]);export{f as __pageData,k as default};
