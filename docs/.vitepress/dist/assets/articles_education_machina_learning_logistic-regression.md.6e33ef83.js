import{_ as e,o as t,c as o,Q as i}from"./chunks/framework.fd8801d6.js";const a="/assets/lr_1.d757091c.png",s="/assets/lr_1.d757091c.png",f=JSON.parse(`{"title":"Logistic Regression: From Odds to Evens in Data's Playground","description":"","frontmatter":{},"headers":[],"relativePath":"articles/education/machina_learning/logistic-regression.md","filePath":"articles/education/machina_learning/logistic-regression.md"}`),l={name:"articles/education/machina_learning/logistic-regression.md"},r=i('<h1 id="logistic-regression-from-odds-to-evens-in-data-s-playground" tabindex="-1">Logistic Regression: From Odds to Evens in Data&#39;s Playground <a class="header-anchor" href="#logistic-regression-from-odds-to-evens-in-data-s-playground" aria-label="Permalink to &quot;Logistic Regression: From Odds to Evens in Data&#39;s Playground&quot;">​</a></h1><blockquote><h1 id="逻辑回归-数据游乐场中的从赔率到偶数" tabindex="-1">逻辑回归：数据游乐场中的从赔率到偶数 <a class="header-anchor" href="#逻辑回归-数据游乐场中的从赔率到偶数" aria-label="Permalink to &quot;逻辑回归：数据游乐场中的从赔率到偶数&quot;">​</a></h1></blockquote><p>Hello data adventurers! 🎒 Today, we&#39;re about to embark on a journey into the realm of logistic regression, a classic yet powerful tool in the data scientist&#39;s toolkit. Through the lens of logistic regression, we&#39;ll explore how we can make sense of the chaotic world of data, especially when we&#39;re dealing with binary outcomes - a yes or a no, a win or a loss, a 1 or a 0. 🔄</p><blockquote><p>数据冒险家们，大家好！🎒 今天，我们即将踏上逻辑回归领域的旅程，这是数据科学家工具包中经典而强大的工具。通过逻辑回归的视角，我们将探索如何理解混乱的数据世界，尤其是当我们处理二元结果时——是或否、赢或输、1 或 0。🔄</p></blockquote><h2 id="history-历史" tabindex="-1">History 历史 <a class="header-anchor" href="#history-历史" aria-label="Permalink to &quot;History 历史&quot;">​</a></h2><p>Our tale begins in the early 19th century with a genius mathematician from Belgium named Pierre François Verhulst. In 1838, Verhulst introduced the world to the logistic function through his publication, &quot;Correspondance mathématique et physique&quot;. The logistic function was like a key that could unlock the complexity of growth processes, especially populations. 🌍🔐</p><blockquote><p>我们的故事始于 19 世纪初，一位名叫皮埃尔·弗朗索瓦·韦尔斯特 （Pierre François Verhulst） 的比利时天才数学家。1838 年，Verhulst 通过他的出版物“Correspondance mathématique et physique”向世界介绍了物流功能。物流功能就像一把钥匙，可以解开增长过程的复杂性，尤其是人口。🌍🔐</p></blockquote><p>Fast forward to the 20th century, the baton of logistic regression was picked up by Joseph Berkson. He modernized logistic regression, making it a staple in the statistical realm from 1944 onwards. Berkson was the one who coined the term &quot;logit&quot;, which is like the magic spell that powers logistic regression. 🪄✨</p><blockquote><p>快进到20世纪，约瑟夫·伯克森（Joseph Berkson）接过了逻辑回归的接力棒。他从1944年开始使逻辑回归现代化，使其成为统计领域的主要内容。伯克森是“logit”一词的创造者，它就像是推动逻辑回归的魔咒。🪄✨</p></blockquote><p>Initially, logistic regression found its playground in the biological sciences, helping researchers make sense of binary outcomes like survival or demise of species based on various factors. However, it wasn’t long before social scientists adopted this magical tool to predict categorical outcomes in their own fields. 🧪📊</p><blockquote><p>最初，逻辑回归在生物科学中找到了它的游乐场，帮助研究人员根据各种因素理解二元结果，如物种的生存或消亡。然而，没过多久，社会科学家就采用了这种神奇的工具来预测他们自己领域的分类结果。🧪📊</p></blockquote><blockquote></blockquote><p>With its roots deeply embedded in history, logistic regression now serves as a bridge between the mathematical and the empirical, enabling us to navigate the binary landscapes of our data-driven world. 🌉</p><blockquote><p>逻辑回归深深植根于历史，现在成为数学和实证之间的桥梁，使我们能够驾驭数据驱动世界的二元景观。🌉</p></blockquote><p>Now that we&#39;ve skimmed the surface of its rich history, are you ready to dive into the mechanism that drives logistic regression? 🤿</p><blockquote><p>现在我们已经略过了其丰富历史的表面，您准备好深入了解驱动逻辑回归的机制了吗？🤿</p></blockquote><h2 id="how-it-works-工作原理" tabindex="-1">How it Works 工作原理 <a class="header-anchor" href="#how-it-works-工作原理" aria-label="Permalink to &quot;How it Works 工作原理&quot;">​</a></h2><p>Logistic regression is like that friendly guide that helps us trek through the binary jungles of data. At its core, it&#39;s a statistical model used to estimate the probability of a binary outcome based on one or more independent variables. 🎲🌿</p><blockquote><p>逻辑回归就像那个友好的指南，帮助我们在数据的二元丛林中跋涉。它的核心是一个统计模型，用于根据一个或多个自变量估计二元结果的概率。🎲🌿</p></blockquote><p>Logistic regression estimates the probability of an event occurring (like casting a vote or identifying a spam email) based on a dataset of independent variables. Unlike linear regression, which predicts a continuous outcome, logistic regression predicts the probability of a discrete outcome, which is mapped to a binary value (0 or 1, Yes or No). The beauty of logistic regression lies in its simplicity and the way it bounds the outcome between 0 and 1, thanks to the logistic function (also known as the sigmoid function):</p><blockquote><p>逻辑回归根据自变量数据集估计事件发生（如投票或识别垃圾邮件）的概率。与预测连续结果的线性回归不同，逻辑回归预测离散结果的概率，该结果映射到二进制值（0 或 1，是或否）。逻辑回归的美妙之处在于它的简单性以及它将结果限制在 0 和 1 之间的方式，这要归功于逻辑函数（也称为 sigmoid 函数）：</p></blockquote><p><img src="'+a+'" alt="Alt text"></p><p>Here, 这里</p><ul><li>P(Y=1) is the probability of the binary outcome being 1.</li></ul><blockquote><ul><li>P（Y=1） 是二元结果为 1 的概率。</li></ul></blockquote><ul><li>beta_0, beta_1, ldots, beta_n are the coefficients that need to be estimated from the data.</li></ul><blockquote><ul><li>beta_0、beta_1、ldots beta_n 是需要从数据中估计的系数。</li></ul></blockquote><ul><li>X_1, X_n are the independent variables.</li></ul><blockquote><ul><li>X_1，X_n是自变量。</li></ul></blockquote><p>Imagine you&#39;re at a game show, and based on certain characteristics (like your age, the number of game shows you&#39;ve attended before, and the color of shirt you&#39;re wearing), the host is trying to predict whether you&#39;ll choose Door #1 or Door #2. Logistic regression is like the host&#39;s educated guessing game, where the host evaluates the likelihood of you choosing Door #1 based on the characteristics you exhibit. 🚪🤔</p><blockquote><p>想象一下，你正在参加一个游戏节目，根据某些特征（比如你的年龄、你以前参加的游戏节目的数量以及你穿的衬衫的颜色），主持人试图预测你会选择门 #1 还是门 #2。逻辑回归就像主持人受过教育的猜谜游戏，主持人根据你表现出的特征来评估你选择门 #1 的可能性。🚪🤔</p></blockquote><h2 id="the-algorithm-算法" tabindex="-1">The Algorithm 算法 <a class="header-anchor" href="#the-algorithm-算法" aria-label="Permalink to &quot;The Algorithm 算法&quot;">​</a></h2><p>Venturing into the algorithmic heart of logistic regression is like understanding the recipe that cooks up our binary predictions. 🍲 Let&#39;s dissect the steps in a simplistic manner:</p><blockquote><p>涉足逻辑回归的算法核心，就像理解了我们二元预测的配方一样。🍲 让我们以简单的方式剖析这些步骤：</p></blockquote><ol><li><strong>Collection of Data</strong>: Gather the data that holds the features (independent variables) and the target variable (the binary outcome we want to predict).</li></ol><blockquote><ol><li><strong>数据收集</strong>：收集包含特征（自变量）和目标变量（我们要预测的二元结果）的数据。</li></ol></blockquote><ol start="2"><li><strong>Initialization</strong>: Set initial values for the coefficients beta_0, beta_1, ldots, beta_n.</li></ol><blockquote><ol start="2"><li><strong>初始化</strong>：设置系数 beta_0、beta_1、ldots beta_n 的初始值。</li></ol></blockquote><ol start="3"><li><strong>Calculation of Prediction</strong>: Using the logistic (sigmoid) function, calculate the probability of the binary outcome being 1 for each observation in the data:</li></ol><blockquote><ol start="3"><li><strong>预测的计算</strong>：使用逻辑（sigmoid）函数，计算数据中每个观测值的二元结果为1的概率： <img src="'+s+'" alt="Alt text"></li></ol></blockquote><ol start="4"><li><strong>Evaluation of Log-Likelihood</strong>: Compute the log-likelihood of observing the given set of outcomes with the current coefficients.</li></ol><blockquote><ol start="4"><li><strong>对数似然评估</strong>：计算使用当前系数观察给定结果集的对数似然。</li></ol></blockquote><ol start="5"><li><strong>Update Coefficients</strong>: Update the coefficients to maximize the log-likelihood using an optimization technique like Gradient Descent.</li></ol><blockquote><ol start="5"><li><strong>更新系数</strong>：使用梯度下降等优化技术更新系数以最大化对数似然。</li></ol></blockquote><ol start="6"><li><strong>Convergence Check</strong>: Check if the coefficients have converged (i.e., the changes in the coefficients are negligible), or if the maximum number of iterations has been reached.</li></ol><blockquote><ol start="6"><li><strong>收敛性检查</strong>：检查系数是否收敛（即系数的变化可以忽略不计），或者是否已达到最大迭代次数。</li></ol></blockquote><ol start="7"><li><strong>Model Evaluation</strong>: Evaluate the performance of the logistic regression model using appropriate metrics like accuracy, precision, recall, etc.</li></ol><blockquote><ol start="7"><li><strong>模型评估</strong>：使用适当的指标（如准确率、精确度、召回率等）评估逻辑回归模型的性能。</li></ol></blockquote><h3 id="invasion-of-the-spam-marauders" tabindex="-1">Invasion of the Spam Marauders <a class="header-anchor" href="#invasion-of-the-spam-marauders" aria-label="Permalink to &quot;Invasion of the Spam Marauders&quot;">​</a></h3><blockquote><h3 id="垃圾邮件掠夺者的入侵" tabindex="-1">垃圾邮件掠夺者的入侵 <a class="header-anchor" href="#垃圾邮件掠夺者的入侵" aria-label="Permalink to &quot;垃圾邮件掠夺者的入侵&quot;">​</a></h3></blockquote><p>Once upon a time in the land of Inboxia, there lived a diligent gatekeeper named Logi. Logi had a very important job—to guard the gates of the grand Email Palace against the invasion of Spam Marauders. The Marauders were notorious for crashing the peaceful gatherings of the genuine Email Folks and causing havoc. 🏰🛡️</p><blockquote><p>很久很久以前，在Inboxia的土地上，住着一位名叫Logi的勤奋的守门人。Logi 有一项非常重要的工作——守卫宏伟的电子邮件宫殿的大门，抵御垃圾邮件掠夺者的入侵。掠夺者因破坏真正的电子邮件人民的和平聚会并造成破坏而臭名昭著。🏰🛡️</p></blockquote><p>Logi had a magic scroll named Logistic Regression, bestowed upon by the ancient Statisticians. The scroll had the power to unveil the guise of the Spam Marauders based on certain traits they exhibited. Two traits were particularly telling—their flashy Armor of Capital Letters and the deceptive Links of Deception they carried. 📜✨</p><blockquote><p>Logi 有一个名为 Logistic Regression 的魔法卷轴，由古代统计学家赋予。该卷轴有能力根据垃圾邮件掠夺者表现出的某些特征揭开他们的伪装。有两个特征特别能说明问题——他们华丽的大写字母盔甲和他们携带的欺骗性链接。📜✨</p></blockquote><h3 id="chapter-1-gathering-the-clues" tabindex="-1">Chapter 1: Gathering the Clues <a class="header-anchor" href="#chapter-1-gathering-the-clues" aria-label="Permalink to &quot;Chapter 1: Gathering the Clues&quot;">​</a></h3><blockquote><h3 id="第-1-章-收集线索" tabindex="-1">第 1 章：收集线索 <a class="header-anchor" href="#第-1-章-收集线索" aria-label="Permalink to &quot;第 1 章：收集线索&quot;">​</a></h3></blockquote><p>Before the sun rose every day, Logi would gather all the messages waiting at the gates. Each message carried with it the frequency of flashy armor (capital letters) and whether it bore any Links of Deception. These were recorded as (X_1) and (X_2) in the magic scroll.</p><blockquote><p>每天太阳升起之前，Logi 都会收集所有在门口等待的信息。每条信息都带有华丽盔甲（大写字母）的频率以及它是否带有任何欺骗链接。这些在魔法卷轴中被记录为 \\（X_1\\） 和 \\（X_2\\）。</p></blockquote><h3 id="chapter-2-invoking-the-magic-scroll" tabindex="-1">Chapter 2: Invoking the Magic Scroll <a class="header-anchor" href="#chapter-2-invoking-the-magic-scroll" aria-label="Permalink to &quot;Chapter 2: Invoking the Magic Scroll&quot;">​</a></h3><blockquote><h3 id="第-2-章-召唤魔法卷轴" tabindex="-1">第 2 章：召唤魔法卷轴 <a class="header-anchor" href="#第-2-章-召唤魔法卷轴" aria-label="Permalink to &quot;第 2 章：召唤魔法卷轴&quot;">​</a></h3></blockquote><p>As the dawn broke, Logi would invoke the magic scroll to estimate the probability of each message being a Spam Marauder. The formula whispered by the scroll was:</p><blockquote><p>当黎明破晓时，Logi 会调用魔法卷轴来估计每封邮件是垃圾邮件掠夺者的概率。卷轴低声说出的公式是：</p></blockquote><p>Here, 这里</p><blockquote><p>-P(Y=1) was the probability of a message being a Spam Marauder. -P（Y=1） 是邮件成为垃圾邮件掠夺者的概率。</p></blockquote><ul><li>beta_0, beta_1, beta_2 were the mystical coefficients that the scroll would learn from the data.</li></ul><blockquote><ul><li>beta_0，beta_1，beta_2是滚动从数据中学习的神秘系数。</li></ul></blockquote><h3 id="chapter-3-learning-from-the-mystical-coefficients" tabindex="-1">Chapter 3: Learning from the Mystical Coefficients <a class="header-anchor" href="#chapter-3-learning-from-the-mystical-coefficients" aria-label="Permalink to &quot;Chapter 3: Learning from the Mystical Coefficients&quot;">​</a></h3><blockquote><h3 id="第-3-章-从神秘系数中学习" tabindex="-1">第 3 章：从神秘系数中学习 <a class="header-anchor" href="#第-3-章-从神秘系数中学习" aria-label="Permalink to &quot;第 3 章：从神秘系数中学习&quot;">​</a></h3></blockquote><p>The magic scroll was wise. It would adjust the mystical coefficients to learn from the messages. The scroll wanted to maximize the likelihood of correctly identifying the Spam Marauders. This quest led to a dance of mathematics—the Gradient Descent—where the scroll iteratively adjusted the coefficients to find the best values.</p><blockquote><p>魔法卷轴是明智的。它将调整神秘系数以从消息中学习。该卷轴希望最大限度地提高正确识别垃圾邮件掠夺者的可能性。这种探索导致了数学之舞——梯度下降——滚动滚动迭代调整系数以找到最佳值。</p></blockquote><h3 id="chapter-4-the-verdict-of-the-scroll" tabindex="-1">Chapter 4: The Verdict of the Scroll <a class="header-anchor" href="#chapter-4-the-verdict-of-the-scroll" aria-label="Permalink to &quot;Chapter 4: The Verdict of the Scroll&quot;">​</a></h3><blockquote><h3 id="第-4-章-卷轴的判决" tabindex="-1">第 4 章：卷轴的判决 <a class="header-anchor" href="#第-4-章-卷轴的判决" aria-label="Permalink to &quot;第 4 章：卷轴的判决&quot;">​</a></h3></blockquote><p>With the mystical coefficients finely tuned, the magic scroll would whisper to Logi the likelihood of each message being from the Spam Marauders. If the probability was high, the message was turned away from the gates, ensuring the peaceful gathering of Email Folks remained undisturbed.</p><blockquote><p>随着神秘系数的微调，魔法卷轴会向罗吉低声说出每条消息都来自垃圾邮件掠夺者的可能性。如果可能性很高，则将邮件拒之门外，确保电子邮件人员的和平聚会不受干扰。</p></blockquote><p>Through days and nights, Logi and the magic scroll stood guard, ensuring the nefarious Spam Marauders were kept at bay, and the land of Inboxia remained a haven for genuine interactions. 🌅</p><blockquote><p>在日日夜夜里，Logi 和魔法卷轴守卫着，确保邪恶的垃圾邮件掠夺者被拒之门外，而 Inboxia 的土地仍然是真正互动的避风港。🌅</p></blockquote><p>And thus, through the lens of a whimsical tale, we&#39;ve journeyed through the algorithmic essence of logistic regression in the realm of spam detection.</p><blockquote><p>因此，通过一个异想天开的故事的镜头，我们已经了解了垃圾邮件检测领域逻辑回归的算法本质。</p></blockquote><h2 id="advantages-优点" tabindex="-1">Advantages 优点 <a class="header-anchor" href="#advantages-优点" aria-label="Permalink to &quot;Advantages 优点&quot;">​</a></h2><p>In the enchanted kingdom of Data Science, Logistic Regression is hailed as a valiant knight 🛡️. Here are some virtues that make it a favorite amongst the kingdom&#39;s scholars:</p><blockquote><p>在数据科学的魔法王国中，逻辑回归被誉为英勇的骑士🛡️。以下是一些使它成为王国学者最爱的美德：</p></blockquote><ol><li><strong>Simplicity</strong>: Logistic Regression is like a clear crystal ball 🔮—easy to interpret and fathom. Its essence is not shrouded in enigma, making it a friendly companion on many quests.</li></ol><blockquote><ol><li><strong>简单性</strong>：逻辑回归就像一个透明的水晶球🔮——易于解释和理解。它的本质并不笼罩在谜团中，使其成为许多任务的友好伴侣。</li></ol></blockquote><ol start="2"><li><strong>Efficiency</strong>: It’s a swift steed 🐎 on the computational battleground. Logistic Regression hastens through training with the grace and speed of a coursing river, saving precious time in the ticking hourglass ⏳.</li></ol><blockquote><ol start="2"><li><strong>效率</strong>：它是计算战场上的一匹敏捷的骏马🐎。后勤回归以河流的优雅和速度加快训练速度，在滴答作响的沙漏⏳中节省宝贵的时间。</li></ol></blockquote><ol start="3"><li><strong>Proclivity for Binary Battles</strong>: It thrives in the lands of binary outcomes 🔄. When the battle cry is between ‘Yes’ and ‘No’, Logistic Regression is the chosen champion.</li></ol><blockquote><ol start="3"><li><strong>二元战斗的倾向</strong>：它在二元结果🔄的土地上茁壮成长。当战斗口号介于“是”和“否”之间时，逻辑回归是被选中的冠军。</li></ol></blockquote><ol start="4"><li><strong>Resistance to Overfitting</strong>: With noble allies like regularization, Logistic Regression stands resilient against the trickster curse of overfitting, ensuring the model doesn’t get entranced by the whispers of noisy data 🎭.</li></ol><blockquote><ol start="4"><li><strong>抗过拟合</strong>：有了正则化这样的高贵盟友，Logistic Regression 可以抵御过拟合的骗子诅咒，确保模型不会被嘈杂的数据🎭的窃窃私语所吸引。</li></ol></blockquote><h2 id="disadvantages-缺点" tabindex="-1">Disadvantages 缺点 <a class="header-anchor" href="#disadvantages-缺点" aria-label="Permalink to &quot;Disadvantages 缺点&quot;">​</a></h2><p>Yet, every knight has its Achilles&#39; heel. Here are the trials that Logistic Regression faces:</p><blockquote><p>然而，每个骑士都有其致命弱点。以下是逻辑回归面临的考验：</p></blockquote><ol><li><strong>Curse of Linearity</strong>: It lives under the spell of linearity 📏, assuming a straight-line relationship between the independent variables and the log odds of the dependent variable. This spell binds Logistic Regression when the real-world data desires to dance in the wild rhythm of non-linearity.</li></ol><blockquote><ol><li><strong>线性的诅咒</strong>：它存在于线性📏的魔咒下，假设自变量与因变量的对数几率之间存在直线关系。当真实世界的数据希望在非线性的狂野节奏中跳舞时，这个咒语会绑定逻辑回归。</li></ol></blockquote><ol start="2"><li><strong>Struggles with Many Features</strong>: In the garden of numerous features, our knight may find itself entangled amidst thorns 🌹. If the observations are fewer than the features, Logistic Regression might succumb to overfitting’s deceit.</li></ol><blockquote><ol start="2"><li><strong>与许多特征的斗争</strong>：在众多特征的花园中，我们的骑士可能会发现自己被荆棘🌹缠住了。如果观测值小于特征，则逻辑回归可能会屈服于过拟合的欺骗。</li></ol></blockquote><ol start="3"><li><strong>Binary Vision</strong>: Its gaze is fixed on binary horizons 🌅. When the quest involves multiclass classification, Logistic Regression requires the fellowship of One-vs-Rest to battle valiantly.</li></ol><blockquote><p>3.<strong>二元视觉</strong>：它的目光固定在二元视界🌅上。当任务涉及多职业分类时，逻辑回归需要一对一与休息的团契才能英勇战斗。</p></blockquote><h2 id="applications-应用" tabindex="-1">Applications 应用 <a class="header-anchor" href="#applications-应用" aria-label="Permalink to &quot;Applications 应用&quot;">​</a></h2><p>Armed with the sword of binary classification, Logistic Regression has championed many a cause in the real world:</p><blockquote><p>在二元分类的剑下，逻辑回归在现实世界中支持了许多事业：</p></blockquote><ol><li><strong>Spam Detection</strong>: As narrated in our whimsical tale, Logistic Regression is a vigilant guard against Spam Marauders, ensuring peace in the land of Inboxia 💌.</li></ol><blockquote><ol><li><strong>垃圾邮件检测</strong>：正如我们异想天开的故事中所叙述的那样，逻辑回归是对垃圾邮件掠夺者的警惕守卫，确保 Inboxia 💌 土地的和平。</li></ol></blockquote><ol start="2"><li><strong>Credit Approval</strong>: In the bustling markets of finance, Logistic Regression is the discerning sage that predicts who is worthy of credit approval 💳.</li></ol><blockquote><ol start="2"><li><strong>信用审批</strong>：在熙熙攘攘的金融市场中，Logistic Regression 是预测谁值得授信审💳批的慧眼智者。</li></ol></blockquote><ol start="3"><li><strong>Medical Diagnosis</strong>: In the hallowed halls of healing, Logistic Regression aids in deciphering the runes of disease diagnosis and patient outcome prediction 🩺.</li></ol><blockquote><ol start="3"><li><strong>医学诊断</strong>：在神圣的疗愈殿堂中，逻辑回归有助于破译疾病诊断和患者预后预测🩺的符文。</li></ol></blockquote><ol start="4"><li><strong>Customer Churn Prediction</strong>: Amidst the lively market squares, it lends its foresight in distinguishing the loyal patrons from the fleeting ones, aiding the merchants in nurturing lasting bonds 🤝.</li></ol><blockquote><ol start="4"><li><strong>客户流失预测</strong>：在热闹的集市广场中，它具有远见卓识，可以区分忠实的顾客和转瞬即逝的顾客，帮助商家建立持久的联系🤝。</li></ol></blockquote><h2 id="tl-dr" tabindex="-1">TL;DR <a class="header-anchor" href="#tl-dr" aria-label="Permalink to &quot;TL;DR&quot;">​</a></h2><p>In the whimsical kingdom of Data Science, Logistic Regression emerges as a valiant knight, guarding the realms of binary classification with honor. Its sword of simplicity and shield of efficiency make it a beloved champion. Yet, the knight faces trials with the Curse of Linearity and the entangling garden of numerous features. Despite these challenges, Logistic Regression valiantly battles in real-world quests, from keeping the nefarious Spam Marauders at bay in the peaceful land of Inboxia, to aiding the discerning sages in finance and the healing seers in healthcare. Our knight’s tale is an ode to the enduring legacy of logistic regression in the ever-evolving landscape of data science. 🛡️⚔️🎇</p><blockquote><p>在异想天开的数据科学王国中，Logistic Regression 以英勇的骑士身份出现，光荣地守卫着二元分类领域。它的简单之剑和效率之盾使它成为深受喜爱的冠军。然而，骑士面临着线性诅咒和众多特征的纠缠花园的考验。尽管面临这些挑战，Logistic Regression 在现实世界的任务中仍然英勇作战，从在和平的 Inboxia 土地上阻止邪恶的垃圾邮件掠夺者，到帮助金融领域的敏锐圣人和医疗保健领域的治愈先知。我们的骑士故事是对数据科学不断发展的领域中逻辑回归的持久遗产的颂歌。🛡️⚔️🎇</p></blockquote><h2 id="vocabulary-list-词汇表" tabindex="-1">Vocabulary List 词汇表 <a class="header-anchor" href="#vocabulary-list-词汇表" aria-label="Permalink to &quot;Vocabulary List 词汇表&quot;">​</a></h2><ul><li><strong>Logistic Regression</strong>: A statistical model used for binary classification, estimating the probability of an event occurrence based on one or more independent variables.</li></ul><blockquote><ul><li><strong>Logistic Regression</strong>：用于二元分类的统计模型，基于一个或多个自变量估计事件发生的概率。</li></ul></blockquote><ul><li><strong>Binary Classification</strong>: The task of classifying the elements of a given set into two groups based on a classification rule.</li></ul><blockquote><ul><li><strong>二元分类</strong>：根据分类规则将给定集合的元素分类为两组的任务。</li></ul></blockquote><ul><li><strong>Logit</strong>: The function used in logistic regression to squeeze the probability estimates between 0 and 1.</li></ul><blockquote><ul><li><strong>Logit</strong>：逻辑回归中使用的函数，用于将概率估计值压缩到 0 到 1 之间。</li></ul></blockquote><ul><li><strong>Gradient Descent</strong>: An optimization algorithm used to minimize some function by iteratively moving in the direction of steepest decrease.</li></ul><blockquote><ul><li><strong>梯度下降</strong>：一种优化算法，用于通过向最陡峭的下降方向迭代移动来最小化某些功能。</li></ul></blockquote><ul><li><strong>Regularization</strong>: A technique used to prevent overfitting by adding a penalty term to the loss function.</li></ul><blockquote><ul><li><strong>正则化</strong>：一种通过向损失函数添加惩罚项来防止过拟合的技术。</li></ul></blockquote><ul><li><strong>Overfitting</strong>: A modeling error that occurs when a function is too closely aligned to a limited set of data points.</li></ul><blockquote><ul><li><strong>过拟合</strong>：当函数与一组有限的数据点过于紧密对齐时发生的建模错误。</li></ul></blockquote><ul><li><strong>Multiclass Classification</strong>: The task of classifying the elements of a given set into more than two groups.</li></ul><blockquote><ul><li><strong>多类分类</strong>：将给定集合的元素分类为两组以上的任务。</li></ul></blockquote>',127),n=[r];function c(h,g,u,d,b,p){return t(),o("div",null,n)}const k=e(l,[["render",c]]);export{f as __pageData,k as default};
