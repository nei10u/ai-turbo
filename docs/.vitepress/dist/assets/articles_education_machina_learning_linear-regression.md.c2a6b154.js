import{_ as e,o,c as t,Q as i}from"./chunks/framework.fd8801d6.js";const k=JSON.parse('{"title":"Linear Regression","description":"","frontmatter":{},"headers":[],"relativePath":"articles/education/machina_learning/linear-regression.md","filePath":"articles/education/machina_learning/linear-regression.md"}'),l={name:"articles/education/machina_learning/linear-regression.md"},a=i('<h1 id="linear-regression" tabindex="-1">Linear Regression <a class="header-anchor" href="#linear-regression" aria-label="Permalink to &quot;Linear Regression&quot;">â€‹</a></h1><blockquote><h1 id="çº¿æ€§å›å½’" tabindex="-1">çº¿æ€§å›å½’ <a class="header-anchor" href="#çº¿æ€§å›å½’" aria-label="Permalink to &quot;çº¿æ€§å›å½’&quot;">â€‹</a></h1></blockquote><p>Hey friends! ğŸ‘‹ It&#39;s me, Miss Neura, here today to break down linear regression.</p><blockquote><p>å˜¿ï¼Œæœ‹å‹ä»¬ï¼ğŸ‘‹ æˆ‘ï¼Œçº½æ‹‰å°å§ï¼Œä»Šå¤©åœ¨è¿™é‡Œåˆ†è§£çº¿æ€§å›å½’ã€‚</p></blockquote><p>Now I know &quot;regression&quot; sounds kinda intimidating. ğŸ˜… But it&#39;s really not so bad!</p><blockquote><p>ç°åœ¨æˆ‘çŸ¥é“â€œå›å½’â€å¬èµ·æ¥æœ‰ç‚¹å“äººã€‚ğŸ˜… ä½†çœŸçš„æ²¡é‚£ä¹ˆç³Ÿç³•ï¼</p></blockquote><p>Linear regression is a useful way to model relationships between variables and make predictions. It works by fitting a line to data points. Simple enough, right? ğŸ“</p><blockquote><p>çº¿æ€§å›å½’æ˜¯å¯¹å˜é‡ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡å’Œè¿›è¡Œé¢„æµ‹çš„æœ‰ç”¨æ–¹æ³•ã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯å°†ä¸€æ¡çº¿æ‹Ÿåˆåˆ°æ•°æ®ç‚¹ã€‚å¾ˆç®€å•ï¼Œå¯¹å§ï¼ŸğŸ“</p></blockquote><p>I&#39;ll explain step-by-step how linear regression crunches numbers to find the line of best fit. Stick with me! ğŸ¤“</p><blockquote><p>æˆ‘å°†é€æ­¥è§£é‡Šçº¿æ€§å›å½’å¦‚ä½•å¤„ç†æ•°å­—ä»¥æ‰¾åˆ°æœ€ä½³æ‹Ÿåˆçº¿ã€‚è·Ÿæˆ‘åœ¨ä¸€èµ·ï¼ğŸ¤“</p></blockquote><p>By the end, you&#39;ll understand the math behind these models and how to build your own. Regression won&#39;t seem so scary anymore. ğŸ˜</p><blockquote><p>æœ€åï¼Œæ‚¨å°†äº†è§£è¿™äº›æ¨¡å‹èƒŒåçš„æ•°å­¦åŸç†ä»¥åŠå¦‚ä½•æ„å»ºè‡ªå·±çš„æ¨¡å‹ã€‚å›å½’ä¼¼ä¹ä¸å†é‚£ä¹ˆå¯æ€•äº†ã€‚ğŸ˜</p></blockquote><p>The key is finding the line that minimizes the distance between predictions and actual data points. We want to squash those residuals!</p><blockquote><p>å…³é”®æ˜¯æ‰¾åˆ°æœ€å°åŒ–é¢„æµ‹å’Œå®é™…æ•°æ®ç‚¹ä¹‹é—´è·ç¦»çš„çº¿ã€‚æˆ‘ä»¬æƒ³å‹æ‰è¿™äº›æ®‹ç•™ç‰©ï¼</p></blockquote><p>Let&#39;s start with some history to see where linear regression originated before we dig into the details. â³</p><blockquote><p>è®©æˆ‘ä»¬ä»ä¸€äº›å†å²å¼€å§‹ï¼Œçœ‹çœ‹çº¿æ€§å›å½’çš„èµ·æºï¼Œç„¶åå†æ·±å…¥ç ”ç©¶ç»†èŠ‚ã€‚â³</p></blockquote><p>Then we can walk through an example to really solidify how it works. Alright, let&#39;s get our learn on! ğŸš€</p><blockquote><p>ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªç¤ºä¾‹æ¥çœŸæ­£å·©å›ºå®ƒçš„å·¥ä½œåŸç†ã€‚å¥½äº†ï¼Œè®©æˆ‘ä»¬å¼€å§‹å­¦ä¹ å§ï¼ğŸš€</p></blockquote><h1 id="history-å†å²" tabindex="-1">History # å†å² <a class="header-anchor" href="#history-å†å²" aria-label="Permalink to &quot;History # å†å²&quot;">â€‹</a></h1><p>Linear regression models have been around for a long time!</p><blockquote><p>çº¿æ€§å›å½’æ¨¡å‹å·²ç»å­˜åœ¨äº†å¾ˆé•¿æ—¶é—´ï¼</p></blockquote><p>The basics were first described way back in the 1800s. ğŸ˜® A mathematician named Francis Galton observed that the heights of parents and their children tended to regress toward the average height in the general population.</p><blockquote><p>æ—©åœ¨ 1800 å¹´ä»£å°±é¦–æ¬¡æè¿°äº†åŸºç¡€çŸ¥è¯†ã€‚ ä¸€ä½åå«å¼—æœ—è¥¿æ–¯Â·é«˜å°”é¡¿çš„æ•°å­¦å®¶è§‚å¯Ÿåˆ°ï¼Œçˆ¶æ¯å’Œå­©å­çš„èº«é«˜å¾€å¾€ä¼šå€’é€€åˆ°æ™®é€šäººç¾¤çš„å¹³å‡èº«é«˜ã€‚ ğŸ˜®</p></blockquote><p>In the early 1900s, more work was done to describe linear regression as we know it today. A famous statistician named Ronald Fisher formalized the model and methods for fitting a line to data.</p><blockquote><p>åœ¨ 1900 å¹´ä»£åˆæœŸï¼Œäººä»¬åšäº†æ›´å¤šçš„å·¥ä½œæ¥æè¿°æˆ‘ä»¬ä»Šå¤©æ‰€çŸ¥é“çš„çº¿æ€§å›å½’ã€‚ä¸€ä½åå«ç½—çº³å¾·Â·è´¹èˆå°”ï¼ˆRonald Fisherï¼‰çš„è‘—åç»Ÿè®¡å­¦å®¶æ­£å¼ç¡®å®šäº†å°†çº¿æ‹Ÿåˆåˆ°æ•°æ®çš„æ¨¡å‹å’Œæ–¹æ³•ã€‚</p></blockquote><p>The first use of automated computation for linear regression came in the 1940s. A cytologist named Gertrude Cox pioneered the use of punch card machines to calculate regression. Talk about old school tech! ğŸ‘µ</p><blockquote><p>çº¿æ€§å›å½’é¦–æ¬¡ä½¿ç”¨è‡ªåŠ¨è®¡ç®—æ˜¯åœ¨ 1940 å¹´ä»£ã€‚ä¸€ä½åå«æ ¼ç‰¹é²å¾·Â·è€ƒå…‹æ–¯ï¼ˆGertrude Coxï¼‰çš„ç»†èƒå­¦å®¶ç‡å…ˆä½¿ç”¨ç©¿å­”å¡æœºæ¥è®¡ç®—å›å½’ã€‚è°ˆè°ˆè€æ´¾æŠ€æœ¯ï¼ğŸ‘µ</p></blockquote><p>Since then, linear regression has been used across many fields from economics to genetics as computing power exploded over the decades.</p><blockquote><p>ä»é‚£æ—¶èµ·ï¼Œéšç€è®¡ç®—èƒ½åŠ›åœ¨å‡ åå¹´çš„çˆ†ç‚¸å¼å¢é•¿ï¼Œçº¿æ€§å›å½’å·²è¢«ç”¨äºä»ç»æµå­¦åˆ°é—ä¼ å­¦çš„è®¸å¤šé¢†åŸŸã€‚</p></blockquote><p>Today, it&#39;s one of the fundamental machine learning and statistics techniques for modeling linear relationships between variables. Phew, linear regression has come a long way! â³</p><blockquote><p>å¦‚ä»Šï¼Œå®ƒæ˜¯ç”¨äºå¯¹å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»è¿›è¡Œå»ºæ¨¡çš„åŸºæœ¬æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡æŠ€æœ¯ä¹‹ä¸€ã€‚å‘¸ï¼Œçº¿æ€§å›å½’å·²ç»èµ°äº†å¾ˆé•¿ä¸€æ®µè·¯ï¼â³</p></blockquote><p>Now that we&#39;ve seen some history, let&#39;s move on to understanding how these models actually work their magic. ğŸ§™â€â™€ï¸ Onward!</p><blockquote><p>ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†ä¸€äº›å†å²ï¼Œè®©æˆ‘ä»¬ç»§ç»­äº†è§£è¿™äº›æ¨¡å‹å®é™…ä¸Šæ˜¯å¦‚ä½•å‘æŒ¥å…¶é­”åŠ›çš„ã€‚ğŸ§™ â™€ï¸ å‘å‰ï¼</p></blockquote><h1 id="how-linear-regression-works-çº¿æ€§å›å½’çš„å·¥ä½œåŸç†" tabindex="-1">How Linear Regression Works çº¿æ€§å›å½’çš„å·¥ä½œåŸç† <a class="header-anchor" href="#how-linear-regression-works-çº¿æ€§å›å½’çš„å·¥ä½œåŸç†" aria-label="Permalink to &quot;How Linear Regression Works çº¿æ€§å›å½’çš„å·¥ä½œåŸç†&quot;">â€‹</a></h1><p>Alright, time to dig into the meat of how linear regression works! ğŸ¥©</p><blockquote><p>å¥½äº†ï¼Œæ˜¯æ—¶å€™æ·±å…¥ç ”ç©¶çº¿æ€§å›å½’çš„å·¥ä½œåŸç†äº†ï¼ğŸ¥©</p></blockquote><p>The goal is to model the relationship between two continuous variables - we&#39;ll call them X and Y.</p><blockquote><p>ç›®æ ‡æ˜¯å¯¹ä¸¤ä¸ªè¿ç»­å˜é‡ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ - æˆ‘ä»¬ç§°å®ƒä»¬ä¸º X å’Œ Yã€‚</p></blockquote><p>X is the independent variable and Y is the dependent variable. Independent variables are the inputs, dependent variables are the outputs.</p><blockquote><p>X æ˜¯è‡ªå˜é‡ï¼ŒY æ˜¯å› å˜é‡ã€‚è‡ªå˜é‡æ˜¯è¾“å…¥ï¼Œå› å˜é‡æ˜¯è¾“å‡ºã€‚</p></blockquote><p>For example, X could be amount spent on ads and Y could be sales generated. Or X could be size of a house and Y could be its price.</p><blockquote><p>ä¾‹å¦‚ï¼ŒX å¯ä»¥æ˜¯å¹¿å‘Šæ”¯å‡ºï¼ŒY å¯ä»¥æ˜¯äº§ç”Ÿçš„é”€å”®é¢ã€‚æˆ–è€… X å¯ä»¥æ˜¯æˆ¿å­çš„å¤§å°ï¼ŒY å¯èƒ½æ˜¯å®ƒçš„ä»·æ ¼ã€‚</p></blockquote><p>Linear regression finds the best straight line (aka linear model) that fits the data points for X and Y.</p><blockquote><p>çº¿æ€§å›å½’æ‰¾åˆ°æ‹Ÿåˆ X å’Œ Y æ•°æ®ç‚¹çš„æœ€ä½³ç›´çº¿ï¼ˆåˆç§°çº¿æ€§æ¨¡å‹ï¼‰ã€‚</p></blockquote><p>This line can be used to predict future values for Y based on a given value of X. Pretty nifty!</p><blockquote><p>è¿™æ¡çº¿å¯ç”¨äºæ ¹æ®ç»™å®šçš„ X å€¼é¢„æµ‹ Y çš„æœªæ¥å€¼ã€‚</p></blockquote><p>The line is characterized by its slope (m) and intercept point (b). Together, they determine where the line is placed.</p><blockquote><p>è¯¥çº¿çš„ç‰¹ç‚¹æ˜¯å…¶æ–œç‡ï¼ˆmï¼‰å’Œæˆªç‚¹ï¼ˆbï¼‰ã€‚å®ƒä»¬å…±åŒå†³å®šäº†çº¿è·¯çš„æ”¾ç½®ä½ç½®ã€‚</p></blockquote><p>The optimal m and b values are found using the least squares method. What does this mean?</p><blockquote><p>ä½¿ç”¨æœ€å°äºŒä¹˜æ³•æ‰¾åˆ°æœ€ä½³ m å’Œ b å€¼ã€‚è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ</p></blockquote><p>It minimizes the distance between data points and the line, also called the residuals. Squashing residuals = happy line. ğŸ˜Š</p><blockquote><p>å®ƒæœ€å°åŒ–äº†æ•°æ®ç‚¹å’Œçº¿ä¹‹é—´çš„è·ç¦»ï¼Œä¹Ÿç§°ä¸ºæ®‹å·®ã€‚æŒ¤å‹æ®‹å·® = å¿«ä¹çº¿ã€‚ğŸ˜Š</p></blockquote><p>This is referred to as the <strong>cost function</strong>, which is a mathematical formula that calculates the total error or &quot;cost&quot; of the current linear regression model. It sums up the residuals (differences between predicted and actual values) for all data points.</p><blockquote><p>è¿™ç§°ä¸ºâ€œæˆæœ¬å‡½æ•°â€ï¼Œå®ƒæ˜¯ä¸€ä¸ªæ•°å­¦å…¬å¼ï¼Œç”¨äºè®¡ç®—å½“å‰çº¿æ€§å›å½’æ¨¡å‹çš„æ€»è¯¯å·®æˆ–â€œæˆæœ¬â€ã€‚å®ƒæ±‡æ€»äº†æ‰€æœ‰æ•°æ®ç‚¹çš„æ®‹å·®ï¼ˆé¢„æµ‹å€¼å’Œå®é™…å€¼ä¹‹é—´çš„å·®å¼‚ï¼‰ã€‚</p></blockquote><p>So in a nutshell, linear regression uses historical data to find the best fit line for continuous variables. This line can then make predictions! âœ¨</p><blockquote><p>å› æ­¤ï¼Œç®€è€Œè¨€ä¹‹ï¼Œçº¿æ€§å›å½’ä½¿ç”¨å†å²æ•°æ®æ¥æŸ¥æ‰¾è¿ç»­å˜é‡çš„æœ€ä½³æ‹Ÿåˆçº¿ã€‚ç„¶åè¿™æ¡çº¿å¯ä»¥åšå‡ºé¢„æµ‹ï¼âœ¨</p></blockquote><h1 id="the-algorithm-ç®—æ³•" tabindex="-1">The Algorithm # ç®—æ³• <a class="header-anchor" href="#the-algorithm-ç®—æ³•" aria-label="Permalink to &quot;The Algorithm # ç®—æ³•&quot;">â€‹</a></h1><p>Let&#39;s say we want to model the relationship between the number of hours studied (x) and test score (y).</p><blockquote><p>å‡è®¾æˆ‘ä»¬æƒ³è¦å¯¹å­¦ä¹ å°æ—¶æ•° ï¼ˆxï¼‰ å’Œè€ƒè¯•åˆ†æ•° ï¼ˆyï¼‰ ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚</p></blockquote><p>We have the following hours studied and test scores:</p><blockquote><p>æˆ‘ä»¬æœ‰ä»¥ä¸‹å­¦ä¹ æ—¶é—´å’Œè€ƒè¯•æˆç»©ï¼š</p></blockquote><p>Hours Studied (x): 2, 4</p><blockquote><p>å­¦ä¹ å°æ—¶æ•° ï¼ˆxï¼‰ï¼š 2ï¼Œ 4</p></blockquote><p>Test Scores (y): 80, 90</p><blockquote><p>è€ƒè¯•æˆç»© ï¼ˆyï¼‰ï¼š 80ï¼Œ 90</p></blockquote><p>Our model is still:</p><blockquote><p>æˆ‘ä»¬çš„æ¨¡å‹ä»ç„¶æ˜¯ï¼š</p></blockquote><p><strong>y = mx + b</strong></p><p>We need to find m and b.</p><blockquote><p>æˆ‘ä»¬éœ€è¦æ‰¾åˆ° m å’Œ bã€‚</p></blockquote><p>Let&#39;s guess m = 5 and b = 75</p><blockquote><p>è®©æˆ‘ä»¬çŒœçŒœ m = 5 å’Œ b = 75</p></blockquote><p>Plugging this into our model and cost function:</p><blockquote><p>å°†å…¶ä»£å…¥æˆ‘ä»¬çš„æ¨¡å‹å’Œæˆæœ¬å‡½æ•°ä¸­ï¼š</p></blockquote><p>For x = 2 hours studied, predicted test score would be y = 5x2 + 75 = 85</p><blockquote><p>å¯¹äº x = 2 å°æ—¶çš„å­¦ä¹ ï¼Œé¢„æµ‹æµ‹è¯•åˆ†æ•°ä¸º y = 5x2 + 75 = 85</p></blockquote><p>Actual y = 80 å®é™… y = 80 Residual = 85 - 80 = 5</p><blockquote><p>æ®‹å·® = 85 - 80 = 5</p></blockquote><p>Now let&#39;s test x = 4 hours studied, predicted y = 5x4 + 75 = 95</p><blockquote><p>ç°åœ¨è®©æˆ‘ä»¬æ£€éªŒ x = 4 å°æ—¶çš„ç ”ç©¶æ—¶é—´ï¼Œé¢„æµ‹ y = 5x4 + 75 = 95</p></blockquote><p>Actual y = 90</p><blockquote><p>å®é™… y = 90</p></blockquote><p>Residual = 95 - 90 = 5</p><blockquote><p>æ®‹å·® = 95 - 90 = 5</p></blockquote><p>Cost function:</p><blockquote><p>æˆæœ¬å‡½æ•°ï¼š The formula to calculate the cost function (or the error between the predicted vs the actual across data points) is <strong>J(m,b) = Î£(y - (mx + b))^2</strong> è®¡ç®—æˆæœ¬å‡½æ•°ï¼ˆæˆ–è·¨æ•°æ®ç‚¹çš„é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹é—´çš„è¯¯å·®ï¼‰çš„å…¬å¼ä¸º <strong>Jï¼ˆmï¼Œbï¼‰ = Î£ï¼ˆy - ï¼ˆmx + bï¼‰ï¼‰^2</strong></p></blockquote><p>So in our example:</p><blockquote><p>å› æ­¤ï¼Œåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼š J(m,b) = 5^2 + 5^2 = 50 Jï¼ˆç±³ï¼Œbï¼‰ = 5^2 + 5^2 = 50</p></blockquote><p>To minimize J, we can tweak m and b. Let&#39;s try:</p><blockquote><p>ä¸ºäº†æœ€å°åŒ– Jï¼Œæˆ‘ä»¬å¯ä»¥è°ƒæ•´ m å’Œ bã€‚è®©æˆ‘ä»¬è¯•è¯•ï¼š</p></blockquote><p>m = 10, b = 60 m = 10ï¼Œ b = 60</p><p>Now:</p><blockquote><p>ç°åœ¨ï¼š For x = 2 hours studied, predicted y = 10x2 + 60 = 80<br> å¯¹äº x = 2 å°æ—¶çš„ç ”ç©¶ï¼Œé¢„æµ‹ y = 10x2 + 60 = 80</p></blockquote><p>Residual = 0</p><blockquote><p>æ®‹å·® = 0</p></blockquote><p>For x = 4, predicted y = 10x4 + 60 = 100</p><blockquote><p>å¯¹äº x = 4ï¼Œé¢„æµ‹å€¼ y = 10x4 + 60 = 100</p></blockquote><p>Residual = 10</p><blockquote><p>æ®‹å·® = 10</p></blockquote><p>Cost function:</p><blockquote><p>æˆæœ¬å‡½æ•°ï¼š J(m,b) = 0^2 + 10^2 = 100 Jï¼ˆç±³ï¼Œbï¼‰ = 0^2 + 10^2 = 100</p></blockquote><p>So we can see tweaking m and b changes the residuals and cost J. We want to find the combo with lowest J.</p><blockquote><p>å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è°ƒæ•´ m å’Œ b ä¼šæ”¹å˜æ®‹å·®å’Œæˆæœ¬ Jã€‚æˆ‘ä»¬æƒ³æ‰¾åˆ° J æœ€ä½çš„ç»„åˆã€‚</p></blockquote><p>In this tiny example, m=10, b=60 minimizes J.</p><blockquote><p>åœ¨è¿™ä¸ªå°ä¾‹å­ä¸­ï¼Œm=10ï¼Œb=60 æœ€å°åŒ– Jã€‚</p></blockquote><p>So our final line is:</p><blockquote><p>æ‰€ä»¥æˆ‘ä»¬çš„æœ€åä¸€è¡Œæ˜¯ï¼š</p></blockquote><p><strong>y = 10x + 60</strong></p><p>The equation y = 10x + 60 represents the best fit straight line relationship between hours studied (x) and test scores (y).</p><blockquote><p>æ–¹ç¨‹ y = 10x + 60 è¡¨ç¤ºå­¦ä¹ æ—¶é—´ ï¼ˆxï¼‰ å’Œè€ƒè¯•åˆ†æ•° ï¼ˆyï¼‰ ä¹‹é—´çš„æœ€ä½³æ‹Ÿåˆç›´çº¿å…³ç³»ã€‚</p></blockquote><p>Specifically: å…·ä½“è¯´æ¥ï¼š</p><ul><li>The slope is 10. This means for every 1 additional hour studied, the model predicts the test score will increase by 10 points.</li></ul><blockquote><ul><li>æ–œç‡ä¸º10ã€‚è¿™æ„å‘³ç€æ¯å¤šç ”ç©¶ 1 å°æ—¶ï¼Œæ¨¡å‹é¢„æµ‹æµ‹è¯•åˆ†æ•°å°†å¢åŠ  10 åˆ†ã€‚</li></ul></blockquote><ul><li>The intercept is 60. This means when 0 hours are studied, the model predicts a test score of 60 points.</li></ul><blockquote><ul><li>æˆªè·ä¸º 60ã€‚è¿™æ„å‘³ç€å½“ç ”ç©¶ 0 å°æ—¶æ—¶ï¼Œæ¨¡å‹é¢„æµ‹æµ‹è¯•åˆ†æ•°ä¸º 60 åˆ†ã€‚</li></ul></blockquote><p>So in plain terms: æ‰€ä»¥ç®€å•æ¥è¯´ï¼š</p><ul><li>Studying more hours corresponds to getting higher test scores</li></ul><blockquote><ul><li>å­¦ä¹ æ—¶é—´è¶Šé•¿ï¼Œè€ƒè¯•æˆç»©å°±è¶Šé«˜</li></ul></blockquote><ul><li>There is a linear relationship where studying 10 additional hours predicts scoring 10 more points</li></ul><blockquote><ul><li>å­˜åœ¨çº¿æ€§å…³ç³»ï¼Œå³å¤šå­¦ä¹  10 å°æ—¶é¢„ç¤ºç€å¤šå¾— 10 åˆ†</li></ul></blockquote><ul><li>Even with 0 hours studied, the model predicts a baseline score of 60 points</li></ul><blockquote><ul><li>å³ä½¿ç ”ç©¶äº† 0 å°æ—¶ï¼Œè¯¥æ¨¡å‹ä¹Ÿé¢„æµ‹åŸºçº¿åˆ†æ•°ä¸º 60 åˆ†</li></ul></blockquote><p>This simple linear model captures the positive correlation between study time and test results. The line models how test scores are expected to improve by a certain amount per hour studied.</p><blockquote><p>è¿™ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹æ•æ‰äº†ç ”ç©¶æ—¶é—´å’Œæµ‹è¯•ç»“æœä¹‹é—´çš„æ­£ç›¸å…³å…³ç³»ã€‚è¯¥çº¿æ¨¡æ‹Ÿäº†æ¯ç ”ç©¶ä¸€å°æ—¶ï¼Œè€ƒè¯•æˆç»©æœ‰æœ›æé«˜ä¸€å®šé‡ã€‚</p></blockquote><h1 id="advantages-ä¼˜åŠ¿" tabindex="-1">Advantages # ä¼˜åŠ¿ <a class="header-anchor" href="#advantages-ä¼˜åŠ¿" aria-label="Permalink to &quot;Advantages # ä¼˜åŠ¿&quot;">â€‹</a></h1><p>Alright, linear regression has some really great advantages worth highlighting! ğŸ˜ƒ</p><blockquote><p>å¥½å§ï¼Œçº¿æ€§å›å½’æœ‰ä¸€äº›éå¸¸å¤§çš„ä¼˜ç‚¹å€¼å¾—å¼ºè°ƒï¼ğŸ˜ƒ</p></blockquote><p>First up, it&#39;s super simple to implement and interpret. â™Ÿï¸ The math isn&#39;t too wild, and the output is an easy-to-understand line equation.</p><blockquote><p>é¦–å…ˆï¼Œå®ƒçš„å®ç°å’Œè§£é‡Šéå¸¸ç®€å•ã€‚â™Ÿï¸ æ•°å­¦ä¸æ˜¯å¤ªç–¯ç‹‚ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªæ˜“äºç†è§£çš„çº¿æ–¹ç¨‹ã€‚</p></blockquote><p>It can also handle multiple independent variables at once! ğŸ¤¯ Just plug them into the equation and away you go. Multi-variable modeling ftw!</p><blockquote><p>å®ƒè¿˜å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªè‡ªå˜é‡ï¼ğŸ¤¯ åªéœ€å°†å®ƒä»¬ä»£å…¥æ–¹ç¨‹å¼ä¸­ï¼Œå°±å¯ä»¥äº†ã€‚å¤šå˜é‡å»ºæ¨¡ftwï¼</p></blockquote><p>Additionally, linear regression works well when there truly is a linear relationship in the data. ğŸ“ˆ It shines when x and y are correlated in a straight-ish line.</p><blockquote><p>æ­¤å¤–ï¼Œå½“æ•°æ®ä¸­ç¡®å®å­˜åœ¨çº¿æ€§å…³ç³»æ—¶ï¼Œçº¿æ€§å›å½’æ•ˆæœå¾ˆå¥½ã€‚ğŸ“ˆ å½“ x å’Œ y åœ¨ä¸€æ¡ç›´çº¿ä¸Šç›¸å…³æ—¶ï¼Œå®ƒä¼šå‘å…‰ã€‚</p></blockquote><p>From a computational side, linear regression has pretty minimal requirements. ğŸ’» It can run efficiently without massive processing power.</p><blockquote><p>ä»è®¡ç®—æ–¹é¢æ¥çœ‹ï¼Œçº¿æ€§å›å½’çš„è¦æ±‚éå¸¸ä½ã€‚ğŸ’» å®ƒå¯ä»¥åœ¨æ²¡æœ‰å¤§é‡å¤„ç†èƒ½åŠ›çš„æƒ…å†µä¸‹é«˜æ•ˆè¿è¡Œã€‚</p></blockquote><p>Lastly, this technique is used across so many fields and has stood the test of time. ğŸ† A true flexible MVP algorithm!</p><blockquote><p>æœ€åï¼Œè¿™ç§æŠ€æœ¯è¢«ç”¨äºè®¸å¤šé¢†åŸŸï¼Œå¹¶ä¸”ç»å—ä½äº†æ—¶é—´çš„è€ƒéªŒã€‚ğŸ† çœŸæ­£çµæ´»çš„MVPç®—æ³•ï¼</p></blockquote><p>Of course, every hero has their kryptonite. Let&#39;s flip to some disadvantages next...ğŸ˜¼</p><blockquote><p>å½“ç„¶ï¼Œæ¯ä¸ªè‹±é›„éƒ½æœ‰è‡ªå·±çš„æ°ªçŸ³ã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬æ¥çœ‹çœ‹ä¸€äº›ç¼ºç‚¹......ğŸ˜¼</p></blockquote><h1 id="disadvantages-ç¼ºç‚¹" tabindex="-1">Disadvantages # ç¼ºç‚¹ <a class="header-anchor" href="#disadvantages-ç¼ºç‚¹" aria-label="Permalink to &quot;Disadvantages # ç¼ºç‚¹&quot;">â€‹</a></h1><p>Alright, linear regression does have some weaknesses we gotta talk about. âš ï¸</p><blockquote><p>å¥½å§ï¼Œçº¿æ€§å›å½’ç¡®å®æœ‰ä¸€äº›æˆ‘ä»¬å¿…é¡»è®¨è®ºçš„å¼±ç‚¹ã€‚âš ï¸</p></blockquote><p>First, it relies on the assumption that the relationship between variables is actually linear. ğŸ“ Messy non-linear data will throw things off.</p><blockquote><p>é¦–å…ˆï¼Œå®ƒä¾èµ–äºå˜é‡ä¹‹é—´çš„å…³ç³»å®é™…ä¸Šæ˜¯çº¿æ€§çš„å‡è®¾ã€‚ğŸ“ æ‚ä¹±æ— ç« çš„éçº¿æ€§æ•°æ®ä¼šè®©äº‹æƒ…å˜å¾—ç³Ÿç³•ã€‚</p></blockquote><p>It&#39;s also prone to overfitting with lots of input features, which can lead to poor generalization on new data. Too much complexity can skew the model. ğŸ¤ª</p><blockquote><p>å®ƒè¿˜å®¹æ˜“å¯¹å¤§é‡è¾“å…¥ç‰¹å¾è¿›è¡Œè¿‡åº¦æ‹Ÿåˆï¼Œè¿™å¯èƒ½å¯¼è‡´å¯¹æ–°æ•°æ®çš„æ³›åŒ–æ€§ä¸ä½³ã€‚è¿‡äºå¤æ‚å¯èƒ½ä¼šä½¿æ¨¡å‹å‡ºç°åå·®ã€‚ğŸ¤ª</p></blockquote><p>Linear regression is sensitive to outliers too. A few weird data points can drastically change that best fit line. ğŸ‘€</p><blockquote><p>çº¿æ€§å›å½’å¯¹å¼‚å¸¸å€¼ä¹Ÿå¾ˆæ•æ„Ÿã€‚ä¸€äº›å¥‡æ€ªçš„æ•°æ®ç‚¹å¯ä»¥å½»åº•æ”¹å˜æœ€ä½³æ‹Ÿåˆçº¿ã€‚ğŸ‘€</p></blockquote><p>And if there&#39;s correlation but not causation between variables, the model might make inaccurate predictions. ğŸ¤¥</p><blockquote><p>å¦‚æœå˜é‡ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ä½†æ²¡æœ‰å› æœå…³ç³»ï¼Œåˆ™æ¨¡å‹å¯èƒ½ä¼šåšå‡ºä¸å‡†ç¡®çš„é¢„æµ‹ã€‚ğŸ¤¥</p></blockquote><p>Finally, linear regression can&#39;t capture non-linear relationships that are inherently more complex. ğŸ¤·â€â™€ï¸ Curves, loops, jumps - nope!</p><blockquote><p>æœ€åï¼Œçº¿æ€§å›å½’æ— æ³•æ•è·æœ¬è´¨ä¸Šæ›´å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚ğŸ¤· â™€ï¸ æ›²çº¿ã€å¾ªç¯ã€è·³è·ƒ - ä¸ï¼</p></blockquote><p>But don&#39;t fret! Many of these disadvantages have workarounds or alternative algorithms. ğŸ’ª</p><blockquote><p>ä½†ä¸è¦æ‹…å¿ƒï¼å…¶ä¸­è®¸å¤šç¼ºç‚¹éƒ½æœ‰å˜é€šæ–¹æ³•æˆ–æ›¿ä»£ç®—æ³•ã€‚ğŸ’ª</p></blockquote><p>Alright, we&#39;re nearing the finish line! Let&#39;s talk about some real-world applications next. ğŸ</p><blockquote><p>å¥½äº†ï¼Œæˆ‘ä»¬å¿«åˆ°ç»ˆç‚¹çº¿äº†ï¼æ¥ä¸‹æ¥æˆ‘ä»¬æ¥è°ˆè°ˆä¸€äº›å®é™…åº”ç”¨ã€‚ğŸ</p></blockquote><h1 id="applications-åº”ç”¨" tabindex="-1">Applications # åº”ç”¨ <a class="header-anchor" href="#applications-åº”ç”¨" aria-label="Permalink to &quot;Applications # åº”ç”¨&quot;">â€‹</a></h1><p>One of the most popular uses of linear regression is predicting housing prices! ğŸ¡</p><blockquote><p>çº¿æ€§å›å½’æœ€æµè¡Œçš„ç”¨é€”ä¹‹ä¸€æ˜¯é¢„æµ‹æˆ¿ä»·ï¼ğŸ¡</p></blockquote><p>Features like square footage, location, age, etc. are used to model price. This helps estimate the value of new properties.</p><blockquote><p>å¹³æ–¹è‹±å°ºã€ä½ç½®ã€å¹´é¾„ç­‰ç‰¹å¾ç”¨äºå¯¹ä»·æ ¼è¿›è¡Œå»ºæ¨¡ã€‚è¿™æœ‰åŠ©äºä¼°è®¡æ–°å±æ€§çš„ä»·å€¼ã€‚</p></blockquote><p>It&#39;s also commonly used in finance to forecast things like revenue, demand, inventory needs and other trends. ğŸ’° Helpful for budgeting!</p><blockquote><p>å®ƒä¹Ÿå¸¸ç”¨äºé‡‘èé¢†åŸŸï¼Œç”¨äºé¢„æµ‹æ”¶å…¥ã€éœ€æ±‚ã€åº“å­˜éœ€æ±‚å’Œå…¶ä»–è¶‹åŠ¿ã€‚ğŸ’° æœ‰åŠ©äºé¢„ç®—ï¼</p></blockquote><p>Economics is another field that leverages linear regression. It can estimate impacts of policy changes on metrics like GDP, unemployment, inflation, and growth. ğŸ“ˆ</p><blockquote><p>ç»æµå­¦æ˜¯å¦ä¸€ä¸ªåˆ©ç”¨çº¿æ€§å›å½’çš„é¢†åŸŸã€‚å®ƒå¯ä»¥ä¼°è®¡æ”¿ç­–å˜åŒ–å¯¹ GDPã€å¤±ä¸šç‡ã€é€šè´§è†¨èƒ€å’Œå¢é•¿ç­‰æŒ‡æ ‡çš„å½±å“ã€‚ğŸ“ˆ</p></blockquote><p>Insurance companies use these models to assess risk factors and set appropriate premiums. Predicting claims helps pricing. ğŸš—</p><blockquote><p>ä¿é™©å…¬å¸ä½¿ç”¨è¿™äº›æ¨¡å‹æ¥è¯„ä¼°é£é™©å› ç´ å¹¶è®¾å®šé€‚å½“çš„ä¿è´¹ã€‚é¢„æµ‹ç´¢èµ”æœ‰åŠ©äºå®šä»·ã€‚ğŸš—</p></blockquote><p>Even fields like healthcare apply linear regression. It helps model the effect of medications, treatments, diets on patient outcomes. ğŸ©º</p><blockquote><p>ç”šè‡³åƒåŒ»ç–—ä¿å¥è¿™æ ·çš„é¢†åŸŸä¹Ÿåº”ç”¨çº¿æ€§å›å½’ã€‚å®ƒæœ‰åŠ©äºæ¨¡æ‹Ÿè¯ç‰©ã€æ²»ç–—ã€é¥®é£Ÿå¯¹æ‚£è€…é¢„åçš„å½±å“ã€‚ğŸ©º</p></blockquote><p>Beyond these examples, linear regression powers many fundamental machine learning algorithms under the hood too!</p><blockquote><p>é™¤äº†è¿™äº›ä¾‹å­ä¹‹å¤–ï¼Œçº¿æ€§å›å½’è¿˜ä¸ºè®¸å¤šåŸºæœ¬çš„æœºå™¨å­¦ä¹ ç®—æ³•æä¾›äº†åŠ¨åŠ›ï¼</p></blockquote><p>It provides a simple baseline for modeling all kinds of relationships between variables. ğŸ”</p><blockquote><p>å®ƒä¸ºå¯¹å˜é‡ä¹‹é—´çš„å„ç§å…³ç³»è¿›è¡Œå»ºæ¨¡æä¾›äº†ä¸€ä¸ªç®€å•çš„åŸºçº¿ã€‚ğŸ”</p></blockquote><h1 id="tl-dr" tabindex="-1">TL;DR <a class="header-anchor" href="#tl-dr" aria-label="Permalink to &quot;TL;DR&quot;">â€‹</a></h1><ul><li>Linear regression is used to model the relationship between two continuous variables. It fits a straight line through data points by minimizing the residuals (differences between predicted and actual y values).</li></ul><blockquote><ul><li>çº¿æ€§å›å½’ç”¨äºå¯¹ä¸¤ä¸ªè¿ç»­å˜é‡ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚å®ƒé€šè¿‡æœ€å°åŒ–æ®‹å·®ï¼ˆé¢„æµ‹å€¼å’Œå®é™… y å€¼ä¹‹é—´çš„å·®å¼‚ï¼‰æ¥æ‹Ÿåˆæ•°æ®ç‚¹çš„ç›´çº¿ã€‚</li></ul></blockquote><ul><li>The line equation is defined as y=mx+b, where m is the slope and b is the intercept. The slope tells us how much y changes for each unit increase in x. The intercept is where the line crosses the y-axis.</li></ul><blockquote><ul><li>ç›´çº¿æ–¹ç¨‹å®šä¹‰ä¸º y=mx+bï¼Œå…¶ä¸­ m æ˜¯æ–œç‡ï¼Œb æ˜¯æˆªè·ã€‚æ–œç‡å‘Šè¯‰æˆ‘ä»¬ x æ¯å¢åŠ ä¸€ä¸ªå•ä½ï¼Œy å°±ä¼šå˜åŒ–å¤šå°‘ã€‚æˆªè·æ˜¯ç›´çº¿ä¸ y è½´ç›¸äº¤çš„ä½ç½®ã€‚</li></ul></blockquote><ul><li>The optimal m and b values are found via gradient descent - iteratively tweaking to reduce error. This results in the line of best fit that most closely models the linear relationship.</li></ul><blockquote><ul><li>é€šè¿‡æ¢¯åº¦ä¸‹é™æ‰¾åˆ°æœ€ä½³ m å’Œ b å€¼ - è¿­ä»£è°ƒæ•´ä»¥å‡å°‘è¯¯å·®ã€‚è¿™æ ·å¯ä»¥ç”Ÿæˆæœ€æ¥è¿‘çº¿æ€§å…³ç³»å»ºæ¨¡çš„æœ€ä½³æ‹Ÿåˆçº¿ã€‚</li></ul></blockquote><ul><li>Key strengths of linear regression include simplicity, interpretability, and modeling linear correlations well. Weaknesses include sensitivity to outliers and inability to capture non-linear trends.</li></ul><blockquote><ul><li>çº¿æ€§å›å½’çš„ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ç®€å•æ€§ã€å¯è§£é‡Šæ€§å’Œå¾ˆå¥½åœ°æ¨¡æ‹Ÿçº¿æ€§ç›¸å…³æ€§ã€‚å¼±ç‚¹åŒ…æ‹¬å¯¹å¼‚å¸¸å€¼çš„æ•æ„Ÿæ€§å’Œæ— æ³•æ•æ‰éçº¿æ€§è¶‹åŠ¿ã€‚</li></ul></blockquote><p>Overall, linear regression is a fundamental machine learning technique for predicting a numerical target based on linear relationships between variables. It continues to serve as a building block for more advanced algorithms too!</p><blockquote><p>æ€»ä½“è€Œè¨€ï¼Œçº¿æ€§å›å½’æ˜¯ä¸€ç§åŸºæœ¬çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œç”¨äºæ ¹æ®å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»é¢„æµ‹æ•°å€¼ç›®æ ‡ã€‚å®ƒä¹Ÿç»§ç»­ä½œä¸ºæ›´é«˜çº§ç®—æ³•çš„æ„å»ºå—ï¼</p></blockquote><h1 id="vocab-list-è¯æ±‡è¡¨" tabindex="-1">Vocab List # è¯æ±‡è¡¨ <a class="header-anchor" href="#vocab-list-è¯æ±‡è¡¨" aria-label="Permalink to &quot;Vocab List # è¯æ±‡è¡¨&quot;">â€‹</a></h1><ul><li>Regression - Modeling the relationship between variables</li></ul><blockquote><ul><li>å›å½’ - å¯¹å˜é‡ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡</li></ul></blockquote><ul><li>Dependent variable - The output variable we&#39;re predicting (y)</li></ul><blockquote><ul><li>å› å˜é‡ - æˆ‘ä»¬é¢„æµ‹çš„è¾“å‡ºå˜é‡ ï¼ˆyï¼‰</li></ul></blockquote><ul><li>Independent variable - The input variable (x)</li></ul><blockquote><ul><li>è‡ªå˜é‡ - è¾“å…¥å˜é‡ ï¼ˆxï¼‰</li></ul></blockquote><ul><li>Slope (m) - How much y changes per change in x</li></ul><blockquote><ul><li>æ–œç‡ ï¼ˆmï¼‰ - æ¯æ¬¡ x å˜åŒ–çš„ y å˜åŒ–é‡</li></ul></blockquote><ul><li>Intercept (b) - Where the line crosses the y-axis</li></ul><blockquote><ul><li>æˆªè· ï¼ˆbï¼‰ - ç›´çº¿ä¸ y è½´ç›¸äº¤çš„ä½ç½®</li></ul></blockquote><ul><li>Residuals - The differences between predicted and actual y</li></ul><blockquote><ul><li>æ®‹å·® - é¢„æµ‹ y å’Œå®é™… y ä¹‹é—´çš„å·®å¼‚</li></ul></blockquote><ul><li>Cost function (J) - Quantifies total error of the line</li></ul><blockquote><ul><li>æˆæœ¬å‡½æ•° ï¼ˆJï¼‰ - é‡åŒ–ç”Ÿäº§çº¿çš„æ€»è¯¯å·®</li></ul></blockquote><ul><li>Gradient descent - Iteratively updates to minimize J</li></ul><blockquote><ul><li>æ¢¯åº¦ä¸‹é™ - è¿­ä»£æ›´æ–°ä»¥æœ€å°åŒ– J</li></ul></blockquote><ul><li>Overfitting - Model matches training data too closely</li></ul><blockquote><ul><li>è¿‡æ‹Ÿåˆ - æ¨¡å‹ä¸è®­ç»ƒæ•°æ®è¿‡äºåŒ¹é…</li></ul></blockquote>',203),s=[a];function n(r,u,p,c,d,h){return o(),t("div",null,s)}const q=e(l,[["render",n]]);export{k as __pageData,q as default};
