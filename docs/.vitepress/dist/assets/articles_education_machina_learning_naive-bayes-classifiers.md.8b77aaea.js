import{_ as e,o,c as t,Q as l}from"./chunks/framework.fd8801d6.js";const d=JSON.parse('{"title":"Naive Bayes Classifiers","description":"","frontmatter":{},"headers":[],"relativePath":"articles/education/machina_learning/naive-bayes-classifiers.md","filePath":"articles/education/machina_learning/naive-bayes-classifiers.md"}'),i={name:"articles/education/machina_learning/naive-bayes-classifiers.md"},a=l('<h1 id="naive-bayes-classifiers" tabindex="-1">Naive Bayes Classifiers <a class="header-anchor" href="#naive-bayes-classifiers" aria-label="Permalink to &quot;Naive Bayes Classifiers&quot;">​</a></h1><blockquote><h1 id="朴素贝叶斯分类器" tabindex="-1">朴素贝叶斯分类器 <a class="header-anchor" href="#朴素贝叶斯分类器" aria-label="Permalink to &quot;朴素贝叶斯分类器&quot;">​</a></h1></blockquote><p>Hey friends! 👋 It&#39;s me, Miss Neura, here today to unpack the Naive Bayes classifier.</p><blockquote><p>嘿，朋友们！👋 是我，Neura小姐，今天在这里解开朴素贝叶斯分类器。</p></blockquote><p>Now I know &quot;naive&quot; doesn&#39;t sound very flattering in the name. 😅 But don&#39;t let that fool you!</p><blockquote><p>现在我知道“天真”这个名字听起来不是很讨人喜欢。😅 但不要让它欺骗你！</p></blockquote><p>Naive Bayes is actually a super simple yet powerful algorithm for classification tasks like spam detection and sentiment analysis. 🎉</p><blockquote><p>朴素贝叶斯实际上是一种超级简单但功能强大的算法，用于垃圾邮件检测和情感分析等分类任务。🎉</p></blockquote><p>It works by calculating conditional probabilities based on Bayes&#39; theorem and an assumption of independence between features.</p><blockquote><p>它的工作原理是根据贝叶斯定理和特征之间独立性的假设来计算条件概率。</p></blockquote><p>I know that sounds a little math-y, but stick with me! 🤓 I&#39;ll break down Bayes and the &quot;naive&quot; assumption piece by piece in easy-to-understand terms.</p><blockquote><p>我知道这听起来有点数学，但请坚持下去！🤓 我将用通俗易懂的术语逐一分解贝叶斯和“幼稚”假设。</p></blockquote><p>By the end, you&#39;ll have a clear understanding of how Naive Bayes ingests data to make predictions for categorical variables. 📈 The key is maximizing those probabilities!</p><blockquote><p>最后，您将清楚地了解朴素贝叶斯如何摄取数据以对分类变量进行预测。📈 关键是最大化这些概率！</p></blockquote><p>Let&#39;s start with a quick history lesson to see where Naive Bayes originated before we dive into the nitty gritty details. ⏳</p><blockquote><p>让我们从一堂快速的历史课开始，看看朴素贝叶斯的起源，然后再深入研究细节。⏳</p></blockquote><h2 id="bayes-history-贝叶斯历史" tabindex="-1">Bayes History 贝叶斯历史 <a class="header-anchor" href="#bayes-history-贝叶斯历史" aria-label="Permalink to &quot;Bayes History 贝叶斯历史&quot;">​</a></h2><p>The original Bayes&#39; theorem dates back to the 1700s when Thomas Bayes first described it.</p><blockquote><p>最初的贝叶斯定理可以追溯到 1700 年代，当时托马斯·贝叶斯首次描述它。</p></blockquote><p>The theorem provided a way to calculate conditional probabilities.</p><blockquote><p>该定理提供了一种计算条件概率的方法。</p></blockquote><p>It laid the foundation for understanding evidence-based statistics and probabilistic reasoning.</p><blockquote><p>它为理解循证统计和概率推理奠定了基础。</p></blockquote><p>Over the years, Bayes&#39; theorem became an important tool across fields like economics, medicine, and computing.</p><blockquote><p>多年来，贝叶斯定理成为经济学、医学和计算等领域的重要工具。</p></blockquote><p>Fast forward to the 1960s - researchers started extending Bayes for classifying data in machine learning.</p><blockquote><p>快进到 1960 年代，研究人员开始扩展贝叶斯，用于对机器学习中的数据进行分类。</p></blockquote><p>But it took high levels of computation to estimate the probabilities needed.</p><blockquote><p>但是需要高水平的计算来估计所需的概率。</p></blockquote><p>Then in the 1990s, the &quot;naive&quot; conditional independence assumption dramatically simplified calculations. 💡</p><blockquote><p>然后在 1990 年代，“幼稚”的条件独立性假设大大简化了计算。💡</p></blockquote><p>This breakthrough yielded the Naive Bayes classifier algorithm we know and love today! 🥰</p><blockquote><p>这一突破产生了我们今天所熟知和喜爱的朴素贝叶斯分类器算法！🥰</p></blockquote><p>Now let&#39;s dive into exactly how Naive Bayes works its probabilistic magic! 🎩 ✨</p><blockquote><p>现在让我们深入了解朴素贝叶斯是如何发挥其概率魔力的！🎩 ✨</p></blockquote><p>How Naive Bayes Works 朴素贝叶斯的工作原理 The &quot;naive&quot; in Naive Bayes comes from an assumption - all the “features” we use are totally independent from each other! ✋🤚</p><blockquote><p>朴素贝叶斯中的“幼稚”来自一个假设——我们使用的所有“特征”都是完全独立的！✋🤚</p></blockquote><p>For example, say we&#39;re building a spam filter using words in the email as features. 📧</p><blockquote><p>例如，假设我们正在使用电子邮件中的字词作为功能来构建垃圾邮件过滤器。📧</p></blockquote><p>The naive assumption means the word &quot;free&quot; appearing has nothing to do with the word &quot;money&quot; appearing. 💰</p><blockquote><p>幼稚的假设意味着“免费”一词的出现与“金钱”一词的出现无关。💰</p></blockquote><p>In the real world, this is often false - spam emails tend to have multiple sketchy words together. 😬</p><blockquote><p>在现实世界中，这通常是错误的——垃圾邮件往往将多个粗略的单词放在一起。😬</p></blockquote><p>But it makes the math so much easier! 😅 We just calculate the probability of each word on its own.</p><blockquote><p>但它使数学变得如此容易！😅 我们只是单独计算每个单词的概率。</p></blockquote><p>To classify an email as spam or not spam, we:</p><blockquote><p>要将电子邮件归类为垃圾邮件或非垃圾邮件，我们：</p></blockquote><p>1️⃣ Find the base rate of spam emails (the prior probability of spam)</p><blockquote><p>1️⃣ 找到垃圾邮件的基本率（垃圾邮件的先验概率）</p></blockquote><p>2️⃣ Calculate the probability of each word appearing in spam emails and not spam emails (the likelihoods)</p><blockquote><p>2️⃣ 计算每个单词出现在垃圾邮件中而不是垃圾邮件中的概率（可能性）</p></blockquote><p>3️⃣ Use Bayes&#39; theorem to multiply these together and get the <strong>posterior probability</strong> that the email is spam</p><blockquote><p>3️⃣ 使用贝叶斯定理将这些相乘，得到电子邮件是垃圾邮件的后验概率</p></blockquote><p>Posterior = Prior x Likelihood1 x Likelihood2 x Likelihood3... 🧮</p><blockquote><p>后验 = 先验 x 似然 1 x 似然 2 x 似然 3...🧮</p></blockquote><p>4️⃣ Compare the posterior probability of spam vs not spam</p><blockquote><p>4️⃣ 比较垃圾邮件与非垃圾邮件的后验概率</p></blockquote><p>Whichever posterior is higher tells us how to classify the email! 💌</p><blockquote><p>无论哪个后部较高，都会告诉我们如何对电子邮件进行分类！💌</p></blockquote><p>So in a nutshell:</p><blockquote><p>简而言之：</p></blockquote><ul><li>Assume feature independence to make the math easy 💪</li></ul><blockquote><ul><li>假设特征独立性，使数学运算变得简单 💪</li></ul></blockquote><ul><li>Calculate prior and likelihoods across features 📈</li></ul><blockquote><ul><li>计算要素📈的先验和似然</li></ul></blockquote><ul><li>Multiply to find posterior probabilities 🧮</li></ul><blockquote><ul><li>乘以求后验概率 🧮</li></ul></blockquote><ul><li>Classify based on the highest posterior! 🏆</li></ul><blockquote><ul><li>根据最高后部进行分类！🏆</li></ul></blockquote><h2 id="the-algorithm-算法" tabindex="-1">The Algorithm 算法 <a class="header-anchor" href="#the-algorithm-算法" aria-label="Permalink to &quot;The Algorithm 算法&quot;">​</a></h2><p>Let&#39;s walk through the key steps of the Naive Bayes algorithm to see the math in action.</p><blockquote><p>让我们来看看朴素贝叶斯算法的关键步骤，看看数学的实际应用。</p></blockquote><p>We&#39;ll use a simple example trying to classify emails as spam or not spam based on 2 keyword features: contains &quot;free&quot; and contains &quot;money&quot;.</p><blockquote><p>我们将使用一个简单的示例，尝试根据 2 个关键字特征将电子邮件分类为垃圾邮件或非垃圾邮件：包含“免费”和包含“金钱”。</p></blockquote><ul><li><strong>1️⃣ Gather your training data</strong></li></ul><blockquote><ul><li><strong>1️⃣ 收集训练数据</strong></li></ul></blockquote><p>We need a training set with emails labeled as spam or not spam to start. Let&#39;s say we have 100 emails:</p><blockquote><p>我们需要一个训练集，其中包含标记为垃圾邮件或非垃圾邮件的电子邮件才能开始。假设我们有 100 封电子邮件：</p></blockquote><ul><li><strong>20 are spam</strong></li></ul><blockquote><ul><li><strong>20 是垃圾邮件</strong></li></ul></blockquote><ul><li><strong>80 are not spam</strong></li></ul><blockquote><ul><li><strong>80 不是垃圾邮件</strong></li></ul></blockquote><ul><li><strong>2️⃣ Calculate the prior probabilities</strong></li></ul><blockquote><ul><li><strong>2️⃣ 计算先验概率</strong></li></ul></blockquote><p>The prior probability of an email being spam P(spam) is 20/100 or 0.2</p><blockquote><p>电子邮件是垃圾邮件 P（垃圾邮件）的先验概率是 20/100 或 0.2</p></blockquote><p>The prior probability of not spam P(not spam) is 80/100 or 0.8</p><blockquote><p>不垃圾邮件 P（非垃圾邮件）的先验概率为 80/100 或 0.8</p></blockquote><p>These are our base rates before seeing any email features.</p><blockquote><p>这些是我们在看到任何电子邮件功能之前的基本费率。</p></blockquote><ul><li><strong>3️⃣ Calculate the likelihood probabilities</strong></li></ul><blockquote><ul><li><strong>3️⃣ 计算似然概率</strong></li></ul></blockquote><p>Let&#39;s say in the training data:</p><blockquote><p>假设在训练数据中：</p></blockquote><p>15 of the 20 spam emails contain the word &quot;free&quot;</p><blockquote><p>20 封垃圾邮件中有 15 封包含“免费”一词 5 of the 80 NOT spam emails contain &quot;free&quot; 80 封非垃圾邮件中有 5 封包含“免费” So the likelihood P(&quot;free&quot;|spam) is 15/20 = 0.75 所以可能性 P（“free”|spam） 是 15/20 = 0.75</p></blockquote><p>And P(&quot;free&quot;|not spam) is 5/80 = 0.0625</p><blockquote><p>P（“free”|not spam） 为 5/80 = 0.0625</p></blockquote><p>We then do the same for the &quot;money&quot; feature.</p><blockquote><p>然后，我们对“金钱”功能执行相同的操作。</p></blockquote><ul><li><strong>4️⃣ Multiply likelihoods and prior to get posteriors</strong></li></ul><blockquote><ul><li><strong>4️⃣ 乘以可能性和获得后验之前</strong></li></ul></blockquote><p>For an email with &quot;free&quot; and &quot;money&quot;, the posterior probabilities are:</p><blockquote><p>对于包含“免费”和“金钱”的电子邮件，后验概率为：</p></blockquote><p>P(spam|&quot;free&quot;,&quot;money&quot;) = P(spam) x P(&quot;free&quot;|spam) x P(&quot;money&quot;|spam)</p><blockquote><p>P（垃圾邮件|”free“，”money“） = P（垃圾邮件） x P（”free“|垃圾邮件） x P（”money“|垃圾邮件）</p></blockquote><p>P(not spam|&quot;free&quot;, &quot;money&quot;) = P(not spam) x P(&quot;free&quot;|not spam) x P(&quot;money&quot;|not spam)</p><blockquote><p>P（不是垃圾邮件|”free“， ”money“） = P（非垃圾邮件） x P（”free“|非垃圾邮件） x P（”money“|非垃圾邮件）</p></blockquote><ul><li><strong>5️⃣ Classify based on highest posterior</strong></li></ul><blockquote><ul><li><strong>5️⃣ 根据最高后验进行分类</strong></li></ul></blockquote><p>If P(spam|&quot;free&quot;,&quot;money&quot;) is higher, we classify the email as spam!</p><blockquote><p>如果 P（spam|”free“，”money“） 更高，我们将电子邮件归类为垃圾邮件！</p></blockquote><h2 id="the-advantages-优势" tabindex="-1">The Advantages 优势 <a class="header-anchor" href="#the-advantages-优势" aria-label="Permalink to &quot;The Advantages 优势&quot;">​</a></h2><p><strong>Fast and simple ⚡️</strong></p><blockquote><p><strong>快速而简单 ⚡️</strong></p></blockquote><ul><li>The naive assumption dramatically reduces computation time compared to other algorithms.</li></ul><blockquote><ul><li>与其他算法相比，这种朴素的假设大大减少了计算时间。</li></ul></blockquote><ul><li>Training is much quicker than neural networks or SVMs.</li></ul><blockquote><ul><li>训练比神经网络或 SVM 快得多。</li></ul></blockquote><p><strong>Performs well with small data 📊</strong></p><blockquote><p><strong>在处理小数据📊时表现良好</strong></p></blockquote><ul><li>Unlike other algorithms, NB doesn&#39;t need tons of training data to estimate robust probabilities.</li></ul><blockquote><ul><li>与其他算法不同，NB 不需要大量的训练数据来估计稳健的概率。</li></ul></blockquote><ul><li>Can learn from fewer examples and still make decent predictions.</li></ul><blockquote><ul><li>可以从较少的例子中学习，并且仍然做出体面的预测。</li></ul></blockquote><p><strong>Easy to implement 💻</strong></p><blockquote><p><strong>易于实施 💻</strong></p></blockquote><ul><li>The math equations are pretty simple to code up.</li></ul><blockquote><ul><li>数学方程式很容易编码。</li></ul></blockquote><ul><li>Much less programming complexity compared to sophisticated techniques.</li></ul><blockquote><ul><li>与复杂的技术相比，编程的复杂性要低得多。</li></ul></blockquote><p><strong>Interpretable 🕵️‍♀️</strong></p><blockquote><p><strong>解释 🕵️ ♀️</strong></p></blockquote><ul><li>Since NB relies on conditional probabilities, we can inspect what features have the highest correlations.</li></ul><blockquote><ul><li>由于 NB 依赖于条件概率，因此我们可以检查哪些特征具有最高的相关性。</li></ul></blockquote><ul><li>More transparent than black box models.</li></ul><blockquote><ul><li>比黑匣子模型更透明。</li></ul></blockquote><p><strong>Resilient to irrelevant features 💪</strong></p><blockquote><p><strong>对不相关的功能💪具有弹性</strong></p></blockquote><ul><li>Adding unnecessary inputs doesn&#39;t affect the model too much.</li></ul><blockquote><ul><li>添加不必要的输入不会对模型产生太大影响。</li></ul></blockquote><ul><li>Independent probabilities diminish irrelevant relationships.</li></ul><blockquote><ul><li>独立概率减少了不相关的关系。</li></ul></blockquote><h2 id="disadvantages-of-naive-bayes-朴素贝叶斯的缺点" tabindex="-1">Disadvantages of Naive Bayes 朴素贝叶斯的缺点 <a class="header-anchor" href="#disadvantages-of-naive-bayes-朴素贝叶斯的缺点" aria-label="Permalink to &quot;Disadvantages of Naive Bayes 朴素贝叶斯的缺点&quot;">​</a></h2><p><strong>Naive assumption 🤨</strong></p><blockquote><p><strong>幼稚的假设 🤨</strong></p></blockquote><ul><li>Features are usually not completely independent in real-world data.</li></ul><blockquote><ul><li>特征在实际数据中通常不是完全独立的。</li></ul></blockquote><ul><li>Violates assumption and leads to inaccurate probabilities.</li></ul><blockquote><ul><li>违反假设并导致不准确的概率。</li></ul></blockquote><p><strong>Presumes dataset distributions📈</strong></p><blockquote><p><strong>假定数据集分布📈</strong></p></blockquote><ul><li>Algorithm presumes data fits standard distribution shapes like Gaussian.</li></ul><blockquote><ul><li>算法假定数据符合标准分布形状，如高斯分布形状。</li></ul></blockquote><ul><li>Real-world data may not match these expected distributions.</li></ul><blockquote><ul><li>实际数据可能与这些预期分布不匹配。</li></ul></blockquote><p><strong>Prone to overfitting 🤪</strong></p><blockquote><p><strong>容易过拟合 🤪</strong></p></blockquote><ul><li>With lots of features, easy to overfit to the training data.</li></ul><blockquote><ul><li>具有许多功能，易于过度拟合训练数据。</li></ul></blockquote><ul><li>Poor generalization to new data. Too many inputs overspecifies.</li></ul><blockquote><ul><li>对新数据的泛化能力差。输入过多，过度指定。</li></ul></blockquote><p><strong>Metrics difficult to calculate 📉</strong></p><blockquote><p><strong>指标难以计算 📉</strong></p></blockquote><ul><li>Standard classification metrics like precision and recall don&#39;t apply naturally.</li></ul><blockquote><ul><li>精确率和召回率等标准分类指标并不自然适用。</li></ul></blockquote><ul><li>Need to use different performance analysis methods.</li></ul><blockquote><ul><li>需要使用不同的性能分析方法。</li></ul></blockquote><p><strong>Not suitable for complex data 🔮</strong></p><blockquote><p><strong>不适用于复杂数据 🔮</strong></p></blockquote><ul><li>Correlated and nonlinear feature relationships violate independence assumption.</li></ul><blockquote><ul><li>相关和非线性特征关系违反了独立性假设。</li></ul></blockquote><ul><li>Struggles with images, audio, video data.</li></ul><blockquote><ul><li>在图像、音频、视频数据方面苦苦挣扎。</li></ul></blockquote><h2 id="applications-应用" tabindex="-1">Applications 应用 <a class="header-anchor" href="#applications-应用" aria-label="Permalink to &quot;Applications 应用&quot;">​</a></h2><p><strong>Spam filtering 📧</strong></p><blockquote><p><strong>垃圾邮件过滤 📧</strong></p></blockquote><ul><li>Classify emails as spam or not spam based on content features.</li></ul><blockquote><ul><li>根据内容特征将电子邮件分类为垃圾邮件或非垃圾邮件。</li></ul></blockquote><ul><li>The naive assumption performs well enough here.</li></ul><blockquote><ul><li>朴素的假设在这里表现得足够好。</li></ul></blockquote><p><strong>Sentiment analysis 😀😡</strong></p><blockquote><p><strong>情绪分析 😀😡</strong></p></blockquote><ul><li>Determine positive or negative sentiment of texts like reviews.</li></ul><blockquote><ul><li>确定评论等文本的正面或负面情绪。</li></ul></blockquote><ul><li>Independent word probabilities work well.</li></ul><blockquote><ul><li>独立词概率效果很好。</li></ul></blockquote><p><strong>Recommender systems 🛍️</strong></p><blockquote><p><strong>推荐系统 🛍️</strong></p></blockquote><ul><li>Recommend products based on past likes/dislikes and product features.</li></ul><blockquote><ul><li>根据过去的好恶和产品功能推荐产品。</li></ul></blockquote><ul><li>Probabilities help identify preferences.</li></ul><blockquote><ul><li>概率有助于识别偏好。</li></ul></blockquote><p><strong>Text classification 📑</strong></p><blockquote><p><strong>文本分类 📑</strong></p></blockquote><ul><li>Categorize texts into topics based on word probabilities.</li></ul><blockquote><ul><li>根据单词概率将文本分类为主题。</li></ul></blockquote><ul><li>Useful for topic modeling and document organizing.</li></ul><blockquote><ul><li>对于主题建模和文档组织很有用。</li></ul></blockquote><p>**Disease prediction 🩺 **</p><blockquote><p><strong>疾病预测 🩺</strong></p></blockquote><ul><li>Predict presence of disease given diagnostic test outcomes.</li></ul><blockquote><ul><li>根据诊断性测试结果预测疾病的存在。</li></ul></blockquote><ul><li>Test results can be used as independent features.</li></ul><blockquote><ul><li>测试结果可用作独立特征。</li></ul></blockquote><h2 id="tl-dr" tabindex="-1">TL;DR <a class="header-anchor" href="#tl-dr" aria-label="Permalink to &quot;TL;DR&quot;">​</a></h2><ul><li>Naive Bayes is a fast, simple classification algorithm that calculates probabilities based on Bayes&#39; theorem and an independence assumption.</li></ul><blockquote><ul><li>朴素贝叶斯是一种快速、简单的分类算法，它根据贝叶斯定理和独立性假设计算概率。</li></ul></blockquote><ul><li>It performs well on small data where its simplicity is an advantage over more complex methods.</li></ul><blockquote><ul><li>它在小数据上表现良好，与更复杂的方法相比，它的简单性是一个优势。</li></ul></blockquote><ul><li>Best suited for problems like spam, sentiment, and recommendations where some independence between features exists.</li></ul><blockquote><ul><li>最适合于垃圾邮件、情绪和推荐等功能之间存在一定独立性的问题。</li></ul></blockquote><ul><li>Not appropriate for complex, correlated data like images, audio, or video.</li></ul><blockquote><ul><li>不适用于复杂的相关数据，如图像、音频或视频。</li></ul></blockquote><ul><li>Overall, Naive Bayes provides a useful balance of simplicity, speed, and performance!</li></ul><blockquote><ul><li>总体而言，朴素贝叶斯在简单性、速度和性能之间提供了有用的平衡！</li></ul></blockquote><h2 id="vocab-list️-词汇表" tabindex="-1">Vocab List️ 词汇表 <a class="header-anchor" href="#vocab-list️-词汇表" aria-label="Permalink to &quot;Vocab List️ 词汇表&quot;">​</a></h2><p>Bayes&#39; theorem - Defines conditional probability P(A|B) as P(B|A)P(A)/P(B).</p><blockquote><p>贝叶斯定理 - 定义条件概率 P（A|B） 作为 P（B|A）P（A）/P（B）。</p></blockquote><p>Likelihood - Probability of data given a hypothesis, P(D|H).</p><blockquote><p>似然 - 给定假设的数据概率 P（D|H）。</p></blockquote><p>Prior probability - Initial probability before new evidence, P(H).</p><blockquote><p>先验概率 - 新证据之前的初始概率，P（H）。</p></blockquote><p>Posterior probability - Updated probability after new evidence, P(H|D).</p><blockquote><p>后验概率 - 新证据后的更新概率，P（H|D).</p></blockquote><p>Conditional independence - Assumption features are unrelated.</p><blockquote><p>条件独立性 - 假设功能不相关。</p></blockquote><p>Gaussian distribution - Normal distribution shaped like a bell curve.</p><blockquote><p>高斯分布 - 形状像钟形曲线的正态分布。</p></blockquote>',229),s=[a];function u(n,r,p,c,b,q){return o(),t("div",null,s)}const h=e(i,[["render",u]]);export{d as __pageData,h as default};
